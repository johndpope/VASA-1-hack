Score-Based Generative Modeling through Stochastic Differential Equations
Yang Song
Stanford University yangsong@cs.stanford.edu
&Jascha Sohl-Dickstein
Google Brain jaschasd@google.com
&Diederik P. Kingma
Google Brain durk@google.com
\ANDAbhishek Kumar
Google Brain abhishk@google.com
&Stefano Ermon
Stanford University ermon@cs.stanford.edu
&Ben Poole
Google Brain pooleb@google.com
Work partially done during an internship at Google Brain.
Abstract
Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 
1024
Ã—
1024
 images for the first time from a score-based generative model.

1Introduction
Two successful classes of probabilistic generative models involve sequentially corrupting training data with slowly increasing noise, and then learning to reverse this corruption in order to form a generative model of the data. Score matching with Langevin dynamics (SMLD) (Song & Ermon, 2019) estimates the score (i.e., the gradient of the log probability density with respect to data) at each noise scale, and then uses Langevin dynamics to sample from a sequence of decreasing noise scales during generation. Denoising diffusion probabilistic modeling (DDPM) (Sohl-Dickstein et al., 2015; Ho et al., 2020) trains a sequence of probabilistic models to reverse each step of the noise corruption, using knowledge of the functional form of the reverse distributions to make training tractable. For continuous state spaces, the DDPM training objective implicitly computes scores at each noise scale. We therefore refer to these two model classes together as score-based generative models.

Score-based generative models, and related techniques (Bordes et al., 2017; Goyal et al., 2017; Du & Mordatch, 2019), have proven effective at generation of images (Song & Ermon, 2019; 2020; Ho et al., 2020), audio (Chen et al., 2020; Kong et al., 2020), graphs (Niu et al., 2020), and shapes (Cai et al., 2020). To enable new sampling methods and further extend the capabilities of score-based generative models, we propose a unified framework that generalizes previous approaches through the lens of stochastic differential equations (SDEs).

Figure 1:Solving a reverse-time SDE yields a score-based generative model. Transforming data to a simple noise distribution can be accomplished with a continuous-time SDE. This SDE can be reversed if we know the score of the distribution at each intermediate time step, 
âˆ‡
ğ±
log
â¡
ğ‘
ğ‘¡
â€‹
(
ğ±
)
.
Refer to caption
Specifically, instead of perturbing data with a finite number of noise distributions, we consider a continuum of distributions that evolve over time according to a diffusion process. This process progressively diffuses a data point into random noise, and is given by a prescribed SDE that does not depend on the data and has no trainable parameters. By reversing this process, we can smoothly mold random noise into data for sample generation. Crucially, this reverse process satisfies a reverse-time SDE (Anderson, 1982), which can be derived from the forward SDE given the score of the marginal probability densities as a function of time. We can therefore approximate the reverse-time SDE by training a time-dependent neural network to estimate the scores, and then produce samples using numerical SDE solvers. Our key idea is summarized in Fig. 1.

Our proposed framework has several theoretical and practical contributions:

Flexible sampling and likelihood computation:  We can employ any general-purpose SDE solver to integrate the reverse-time SDE for sampling. In addition, we propose two special methods not viable for general SDEs: (i) Predictor-Corrector (PC) samplers that combine numerical SDE solvers with score-based MCMC approaches, such as Langevin MCMC (Parisi, 1981) and HMC (Neal et al., 2011); and (ii) deterministic samplers based on the probability flow ordinary differential equation (ODE). The former unifies and improves over existing sampling methods for score-based models. The latter allows for fast adaptive sampling via black-box ODE solvers, flexible data manipulation via latent codes, a uniquely identifiable encoding, and notably, exact likelihood computation.

Controllable generation:  We can modulate the generation process by conditioning on information not available during training, because the conditional reverse-time SDE can be efficiently estimated from unconditional scores. This enables applications such as class-conditional generation, image inpainting, colorization and other inverse problems, all achievable using a single unconditional score-based model without re-training.

Unified framework:  Our framework provides a unified way to explore and tune various SDEs for improving score-based generative models. The methods of SMLD and DDPM can be amalgamated into our framework as discretizations of two separate SDEs. Although DDPM (Ho et al., 2020) was recently reported to achieve higher sample quality than SMLD (Song & Ermon, 2019; 2020), we show that with better architectures and new sampling algorithms allowed by our framework, the latter can catch upâ€”it achieves new state-of-the-art Inception score (9.89) and FID score (2.20) on CIFAR-10, as well as high-fidelity generation of 
1024
Ã—
1024
 images for the first time from a score-based model. In addition, we propose a new SDE under our framework that achieves a likelihood value of 2.99 bits/dim on uniformly dequantized CIFAR-10 images, setting a new record on this task.

2Background
2.1Denoising score matching with Langevin dynamics (SMLD)
Let 
ğ‘
ğœ
â€‹
(
ğ±
~
âˆ£
ğ±
)
â‰”
ğ’©
â€‹
(
ğ±
~
;
ğ±
,
ğœ
2
â€‹
ğˆ
)
 be a perturbation kernel, and 
ğ‘
ğœ
â€‹
(
ğ±
~
)
â‰”
âˆ«
ğ‘
data
â€‹
(
ğ±
)
â€‹
ğ‘
ğœ
â€‹
(
ğ±
~
âˆ£
ğ±
)
â€‹
d
ğ±
, where 
ğ‘
data
â€‹
(
ğ±
)
 denotes the data distribution. Consider a sequence of positive noise scales 
ğœ
min
=
ğœ
1
<
ğœ
2
<
â‹¯
<
ğœ
ğ‘
=
ğœ
max
. Typically, 
ğœ
min
 is small enough such that 
ğ‘
ğœ
min
â€‹
(
ğ±
)
â‰ˆ
ğ‘
data
â€‹
(
ğ±
)
, and 
ğœ
max
 is large enough such that 
ğ‘
ğœ
max
â€‹
(
ğ±
)
â‰ˆ
ğ’©
â€‹
(
ğ±
;
ğŸ
,
ğœ
max
2
â€‹
ğˆ
)
. Song & Ermon (2019) propose to train a Noise Conditional Score Network (NCSN), denoted by 
ğ¬
ğœ½
â€‹
(
ğ±
,
ğœ
)
, with a weighted sum of denoising score matching (Vincent, 2011) objectives:

ğœ½
âˆ—
=
arg
â€‹
min
ğœ½
â€‹
âˆ‘
ğ‘–
=
1
ğ‘
ğœ
ğ‘–
2
â€‹
ğ”¼
ğ‘
data
â€‹
(
ğ±
)
â€‹
ğ”¼
ğ‘
ğœ
ğ‘–
â€‹
(
ğ±
~
âˆ£
ğ±
)
â€‹
[
âˆ¥
ğ¬
ğœ½
â€‹
(
ğ±
~
,
ğœ
ğ‘–
)
âˆ’
âˆ‡
ğ±
~
log
â¡
ğ‘
ğœ
ğ‘–
â€‹
(
ğ±
~
âˆ£
ğ±
)
âˆ¥
2
2
]
.
(1)
Given sufficient data and model capacity, the optimal score-based model 
ğ¬
ğœ½
âˆ—
â€‹
(
ğ±
,
ğœ
)
 matches 
âˆ‡
ğ±
log
â¡
ğ‘
ğœ
â€‹
(
ğ±
)
 almost everywhere for 
ğœ
âˆˆ
{
ğœ
ğ‘–
}
ğ‘–
=
1
ğ‘
. For sampling, Song & Ermon (2019) run 
ğ‘€
 steps of Langevin MCMC to get a sample for each 
ğ‘
ğœ
ğ‘–
â€‹
(
ğ±
)
 sequentially:

ğ±
ğ‘–
ğ‘š
=
ğ±
ğ‘–
ğ‘š
âˆ’
1
+
ğœ–
ğ‘–
â€‹
ğ¬
ğœ½
âˆ—
â€‹
(
ğ±
ğ‘–
ğ‘š
âˆ’
1
,
ğœ
ğ‘–
)
+
2
â€‹
ğœ–
ğ‘–
â€‹
ğ³
ğ‘–
ğ‘š
,
ğ‘š
=
1
,
2
,
â‹¯
,
ğ‘€
,
(2)
where 
ğœ–
ğ‘–
>
0
 is the step size, and 
ğ³
ğ‘–
ğ‘š
 is standard normal. The above is repeated for 
ğ‘–
=
ğ‘
,
ğ‘
âˆ’
1
,
â‹¯
,
1
 in turn with 
ğ±
ğ‘
0
âˆ¼
ğ’©
â€‹
(
ğ±
âˆ£
ğŸ
,
ğœ
max
2
â€‹
ğˆ
)
 and 
ğ±
ğ‘–
0
=
ğ±
ğ‘–
+
1
ğ‘€
 when 
ğ‘–
<
ğ‘
. As 
ğ‘€
â†’
âˆ
 and 
ğœ–
ğ‘–
â†’
0
 for all 
ğ‘–
, 
ğ±
1
ğ‘€
 becomes an exact sample from 
ğ‘
ğœ
min
â€‹
(
ğ±
)
â‰ˆ
ğ‘
data
â€‹
(
ğ±
)
 under some regularity conditions.

2.2Denoising diffusion probabilistic models (DDPM)
Sohl-Dickstein et al. (2015); Ho et al. (2020) consider a sequence of positive noise scales 
0
<
ğ›½
1
,
ğ›½
2
,
â‹¯
,
ğ›½
ğ‘
<
1
. For each training data point 
ğ±
0
âˆ¼
ğ‘
data
â€‹
(
ğ±
)
, a discrete Markov chain 
{
ğ±
0
,
ğ±
1
,
â‹¯
,
ğ±
ğ‘
}
 is constructed such that 
ğ‘
â€‹
(
ğ±
ğ‘–
âˆ£
ğ±
ğ‘–
âˆ’
1
)
=
ğ’©
â€‹
(
ğ±
ğ‘–
;
1
âˆ’
ğ›½
ğ‘–
â€‹
ğ±
ğ‘–
âˆ’
1
,
ğ›½
ğ‘–
â€‹
ğˆ
)
, and therefore 
ğ‘
ğ›¼
ğ‘–
â€‹
(
ğ±
ğ‘–
âˆ£
ğ±
0
)
=
ğ’©
â€‹
(
ğ±
ğ‘–
;
ğ›¼
ğ‘–
â€‹
ğ±
0
,
(
1
âˆ’
ğ›¼
ğ‘–
)
â€‹
ğˆ
)
, where 
ğ›¼
ğ‘–
â‰”
âˆ
ğ‘—
=
1
ğ‘–
(
1
âˆ’
ğ›½
ğ‘—
)
. Similar to SMLD, we can denote the perturbed data distribution as 
ğ‘
ğ›¼
ğ‘–
â€‹
(
ğ±
~
)
â‰”
âˆ«
ğ‘
data
â€‹
(
ğ±
)
â€‹
ğ‘
ğ›¼
ğ‘–
â€‹
(
ğ±
~
âˆ£
ğ±
)
â€‹
d
ğ±
. The noise scales are prescribed such that 
ğ±
ğ‘
 is approximately distributed according to 
ğ’©
â€‹
(
ğŸ
,
ğˆ
)
. A variational Markov chain in the reverse direction is parameterized with 
ğ‘
ğœ½
â€‹
(
ğ±
ğ‘–
âˆ’
1
|
ğ±
ğ‘–
)
=
ğ’©
â€‹
(
ğ±
ğ‘–
âˆ’
1
;
1
1
âˆ’
ğ›½
ğ‘–
â€‹
(
ğ±
ğ‘–
+
ğ›½
ğ‘–
â€‹
ğ¬
ğœ½
â€‹
(
ğ±
ğ‘–
,
ğ‘–
)
)
,
ğ›½
ğ‘–
â€‹
ğˆ
)
, and trained with a re-weighted variant of the evidence lower bound (ELBO):

ğœ½
âˆ—
=
arg
â€‹
min
ğœ½
â€‹
âˆ‘
ğ‘–
=
1
ğ‘
(
1
âˆ’
ğ›¼
ğ‘–
)
â€‹
ğ”¼
ğ‘
data
â€‹
(
ğ±
)
â€‹
ğ”¼
ğ‘
ğ›¼
ğ‘–
â€‹
(
ğ±
~
âˆ£
ğ±
)
â€‹
[
âˆ¥
ğ¬
ğœ½
â€‹
(
ğ±
~
,
ğ‘–
)
âˆ’
âˆ‡
ğ±
~
log
â¡
ğ‘
ğ›¼
ğ‘–
â€‹
(
ğ±
~
âˆ£
ğ±
)
âˆ¥
2
2
]
.
(3)
After solving Eq. 3 to get the optimal model 
ğ¬
ğœ½
âˆ—
â€‹
(
ğ±
,
ğ‘–
)
, samples can be generated by starting from 
ğ±
ğ‘
âˆ¼
ğ’©
â€‹
(
ğŸ
,
ğˆ
)
 and following the estimated reverse Markov chain as below

ğ±
ğ‘–
âˆ’
1
=
1
1
âˆ’
ğ›½
ğ‘–
â€‹
(
ğ±
ğ‘–
+
ğ›½
ğ‘–
â€‹
ğ¬
ğœ½
âˆ—
â€‹
(
ğ±
ğ‘–
,
ğ‘–
)
)
+
ğ›½
ğ‘–
â€‹
ğ³
ğ‘–
,
ğ‘–
=
ğ‘
,
ğ‘
âˆ’
1
,
â‹¯
,
1
.
(4)
We call this method ancestral sampling, since it amounts to performing ancestral sampling from the graphical model 
âˆ
ğ‘–
=
1
ğ‘
ğ‘
ğœ½
â€‹
(
ğ±
ğ‘–
âˆ’
1
âˆ£
ğ±
ğ‘–
)
. The objective Eq. 3 described here is 
ğ¿
simple
 in Ho et al. (2020), written in a form to expose more similarity to Eq. 1. Like Eq. 1, Eq. 3 is also a weighted sum of denoising score matching objectives, which implies that the optimal model, 
ğ¬
ğœ½
âˆ—
â€‹
(
ğ±
~
,
ğ‘–
)
, matches the score of the perturbed data distribution, 
âˆ‡
ğ±
log
â¡
ğ‘
ğ›¼
ğ‘–
â€‹
(
ğ±
)
. Notably, the weights of the 
ğ‘–
-th summand in Eq. 1 and Eq. 3, namely 
ğœ
ğ‘–
2
 and 
(
1
âˆ’
ğ›¼
ğ‘–
)
, are related to corresponding perturbation kernels in the same functional form: 
ğœ
ğ‘–
2
âˆ
1
/
ğ”¼
â€‹
[
âˆ¥
âˆ‡
ğ±
log
â¡
ğ‘
ğœ
ğ‘–
â€‹
(
ğ±
~
âˆ£
ğ±
)
âˆ¥
2
2
]
 and 
(
1
âˆ’
ğ›¼
ğ‘–
)
âˆ
1
/
ğ”¼
â€‹
[
âˆ¥
âˆ‡
ğ±
log
â¡
ğ‘
ğ›¼
ğ‘–
â€‹
(
ğ±
~
âˆ£
ğ±
)
âˆ¥
2
2
]
.

3Score-based generative modeling with SDEs
Perturbing data with multiple noise scales is key to the success of previous methods. We propose to generalize this idea further to an infinite number of noise scales, such that perturbed data distributions evolve according to an SDE as the noise intensifies. An overview of our framework is given in Fig. 2.

Refer to caption
Figure 2:Overview of score-based generative modeling through SDEs. We can map data to a noise distribution (the prior) with an SDE (Section 3.1), and reverse this SDE for generative modeling (Section 3.2). We can also reverse the associated probability flow ODE (Section 4.3), which yields a deterministic process that samples from the same distribution as the SDE. Both the reverse-time SDE and probability flow ODE can be obtained by estimating the score 
âˆ‡
ğ±
log
â¡
ğ‘
ğ‘¡
â€‹
(
ğ±
)
 (Section 3.3).
3.1Perturbing data with SDEs
Our goal is to construct a diffusion process 
{
ğ±
â€‹
(
ğ‘¡
)
}
ğ‘¡
=
0
ğ‘‡
 indexed by a continuous time variable 
ğ‘¡
âˆˆ
[
0
,
ğ‘‡
]
, such that 
ğ±
â€‹
(
0
)
âˆ¼
ğ‘
0
, for which we have a dataset of i.i.d. samples, and 
ğ±
â€‹
(
ğ‘‡
)
âˆ¼
ğ‘
ğ‘‡
, for which we have a tractable form to generate samples efficiently. In other words, 
ğ‘
0
 is the data distribution and 
ğ‘
ğ‘‡
 is the prior distribution. This diffusion process can be modeled as the solution to an ItÃ´ SDE:

d
â€‹
ğ±
=
ğŸ
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
d
â€‹
ğ‘¡
+
ğ‘”
â€‹
(
ğ‘¡
)
â€‹
d
â€‹
ğ°
,
(5)
where 
ğ°
 is the standard Wiener process (a.k.a., Brownian motion), 
ğŸ
â€‹
(
â‹…
,
ğ‘¡
)
:
â„
ğ‘‘
â†’
â„
ğ‘‘
 is a vector-valued function called the drift coefficient of 
ğ±
â€‹
(
ğ‘¡
)
, and 
ğ‘”
â€‹
(
â‹…
)
:
â„
â†’
â„
 is a scalar function known as the diffusion coefficient of 
ğ±
â€‹
(
ğ‘¡
)
. For ease of presentation we assume the diffusion coefficient is a scalar (instead of a 
ğ‘‘
Ã—
ğ‘‘
 matrix) and does not depend on 
ğ±
, but our theory can be generalized to hold in those cases (see Appendix A). The SDE has a unique strong solution as long as the coefficients are globally Lipschitz in both state and time (Ã˜ksendal, 2003). We hereafter denote by 
ğ‘
ğ‘¡
â€‹
(
ğ±
)
 the probability density of 
ğ±
â€‹
(
ğ‘¡
)
, and use 
ğ‘
ğ‘ 
â€‹
ğ‘¡
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
âˆ£
ğ±
â€‹
(
ğ‘ 
)
)
 to denote the transition kernel from 
ğ±
â€‹
(
ğ‘ 
)
 to 
ğ±
â€‹
(
ğ‘¡
)
, where 
0
â‰¤
ğ‘ 
<
ğ‘¡
â‰¤
ğ‘‡
.

Typically, 
ğ‘
ğ‘‡
 is an unstructured prior distribution that contains no information of 
ğ‘
0
, such as a Gaussian distribution with fixed mean and variance. There are various ways of designing the SDE in Eq. 5 such that it diffuses the data distribution into a fixed prior distribution. We provide several examples later in Section 3.4 that are derived from continuous generalizations of SMLD and DDPM.

3.2Generating samples by reversing the SDE
By starting from samples of 
ğ±
â€‹
(
ğ‘‡
)
âˆ¼
ğ‘
ğ‘‡
 and reversing the process, we can obtain samples 
ğ±
â€‹
(
0
)
âˆ¼
ğ‘
0
. A remarkable result from Anderson (1982) states that the reverse of a diffusion process is also a diffusion process, running backwards in time and given by the reverse-time SDE:

d
â€‹
ğ±
=
[
ğŸ
â€‹
(
ğ±
,
ğ‘¡
)
âˆ’
ğ‘”
â€‹
(
ğ‘¡
)
2
â€‹
âˆ‡
ğ±
log
â¡
ğ‘
ğ‘¡
â€‹
(
ğ±
)
]
â€‹
d
â€‹
ğ‘¡
+
ğ‘”
â€‹
(
ğ‘¡
)
â€‹
d
â€‹
ğ°
Â¯
,
(6)
where 
ğ°
Â¯
 is a standard Wiener process when time flows backwards from 
ğ‘‡
 to 
0
, and 
d
â€‹
ğ‘¡
 is an infinitesimal negative timestep. Once the score of each marginal distribution, 
âˆ‡
ğ±
log
â¡
ğ‘
ğ‘¡
â€‹
(
ğ±
)
, is known for all 
ğ‘¡
, we can derive the reverse diffusion process from Eq. 6 and simulate it to sample from 
ğ‘
0
.

3.3Estimating scores for the SDE
The score of a distribution can be estimated by training a score-based model on samples with score matching (HyvÃ¤rinen, 2005; Song et al., 2019a). To estimate 
âˆ‡
ğ±
log
â¡
ğ‘
ğ‘¡
â€‹
(
ğ±
)
, we can train a time-dependent score-based model 
ğ¬
ğœ½
â€‹
(
ğ±
,
ğ‘¡
)
 via a continuous generalization to Eqs. 1 and 3:

ğœ½
âˆ—
=
arg
â€‹
min
ğœ½
â¡
ğ”¼
ğ‘¡
â€‹
{
ğœ†
â€‹
(
ğ‘¡
)
â€‹
ğ”¼
ğ±
â€‹
(
0
)
â€‹
ğ”¼
ğ±
â€‹
(
ğ‘¡
)
âˆ£
ğ±
â€‹
(
0
)
â€‹
[
âˆ¥
ğ¬
ğœ½
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
,
ğ‘¡
)
âˆ’
âˆ‡
ğ±
â€‹
(
ğ‘¡
)
log
â¡
ğ‘
0
â€‹
ğ‘¡
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
âˆ£
ğ±
â€‹
(
0
)
)
âˆ¥
2
2
]
}
.
(7)
Here 
ğœ†
:
[
0
,
ğ‘‡
]
â†’
â„
>
0
 is a positive weighting function, 
ğ‘¡
 is uniformly sampled over 
[
0
,
ğ‘‡
]
, 
ğ±
â€‹
(
0
)
âˆ¼
ğ‘
0
â€‹
(
ğ±
)
 and 
ğ±
â€‹
(
ğ‘¡
)
âˆ¼
ğ‘
0
â€‹
ğ‘¡
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
âˆ£
ğ±
â€‹
(
0
)
)
. With sufficient data and model capacity, score matching ensures that the optimal solution to Eq. 7, denoted by 
ğ¬
ğœ½
âˆ—
â€‹
(
ğ±
,
ğ‘¡
)
, equals 
âˆ‡
ğ±
log
â¡
ğ‘
ğ‘¡
â€‹
(
ğ±
)
 for almost all 
ğ±
 and 
ğ‘¡
. As in SMLD and DDPM, we can typically choose 
ğœ†
âˆ
1
/
ğ”¼
â€‹
[
âˆ¥
âˆ‡
ğ±
â€‹
(
ğ‘¡
)
log
â¡
ğ‘
0
â€‹
ğ‘¡
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
âˆ£
ğ±
â€‹
(
0
)
)
âˆ¥
2
2
]
. Note that Eq. 7 uses denoising score matching, but other score matching objectives, such as sliced score matching (Song et al., 2019a) and finite-difference score matching (Pang et al., 2020) are also applicable here.

We typically need to know the transition kernel 
ğ‘
0
â€‹
ğ‘¡
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
âˆ£
ğ±
â€‹
(
0
)
)
 to efficiently solve Eq. 7. When 
ğŸ
â€‹
(
â‹…
,
ğ‘¡
)
 is affine, the transition kernel is always a Gaussian distribution, where the mean and variance are often known in closed-forms and can be obtained with standard techniques (see Section 5.5 in SÃ¤rkkÃ¤ & Solin (2019)). For more general SDEs, we may solve Kolmogorovâ€™s forward equation (Ã˜ksendal, 2003) to obtain 
ğ‘
0
â€‹
ğ‘¡
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
âˆ£
ğ±
â€‹
(
0
)
)
. Alternatively, we can simulate the SDE to sample from 
ğ‘
0
â€‹
ğ‘¡
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
âˆ£
ğ±
â€‹
(
0
)
)
 and replace denoising score matching in Eq. 7 with sliced score matching for model training, which bypasses the computation of 
âˆ‡
ğ±
â€‹
(
ğ‘¡
)
log
â¡
ğ‘
0
â€‹
ğ‘¡
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
âˆ£
ğ±
â€‹
(
0
)
)
 (see Appendix A).

3.4Examples: VE, VP SDEs and beyond
The noise perturbations used in SMLD and DDPM can be regarded as discretizations of two different SDEs. Below we provide a brief discussion and relegate more details to Appendix B.

When using a total of 
ğ‘
 noise scales, each perturbation kernel 
ğ‘
ğœ
ğ‘–
â€‹
(
ğ±
âˆ£
ğ±
0
)
 of SMLD corresponds to the distribution of 
ğ±
ğ‘–
 in the following Markov chain:

ğ±
ğ‘–
=
ğ±
ğ‘–
âˆ’
1
+
ğœ
ğ‘–
2
âˆ’
ğœ
ğ‘–
âˆ’
1
2
â€‹
ğ³
ğ‘–
âˆ’
1
,
ğ‘–
=
1
,
â‹¯
,
ğ‘
,
(8)
where 
ğ³
ğ‘–
âˆ’
1
âˆ¼
ğ’©
â€‹
(
ğŸ
,
ğˆ
)
, and we have introduced 
ğœ
0
=
0
 to simplify the notation. In the limit of 
ğ‘
â†’
âˆ
, 
{
ğœ
ğ‘–
}
ğ‘–
=
1
ğ‘
 becomes a function 
ğœ
â€‹
(
ğ‘¡
)
, 
ğ³
ğ‘–
 becomes 
ğ³
â€‹
(
ğ‘¡
)
, and the Markov chain 
{
ğ±
ğ‘–
}
ğ‘–
=
1
ğ‘
 becomes a continuous stochastic process 
{
ğ±
â€‹
(
ğ‘¡
)
}
ğ‘¡
=
0
1
, where we have used a continuous time variable 
ğ‘¡
âˆˆ
[
0
,
1
]
 for indexing, rather than an integer 
ğ‘–
. The process 
{
ğ±
â€‹
(
ğ‘¡
)
}
ğ‘¡
=
0
1
 is given by the following SDE

d
â€‹
ğ±
=
d
â€‹
[
ğœ
2
â€‹
(
ğ‘¡
)
]
d
â€‹
ğ‘¡
â€‹
d
â€‹
ğ°
.
(9)
Likewise for the perturbation kernels 
{
ğ‘
ğ›¼
ğ‘–
â€‹
(
ğ±
âˆ£
ğ±
0
)
}
ğ‘–
=
1
ğ‘
 of DDPM, the discrete Markov chain is

ğ±
ğ‘–
=
1
âˆ’
ğ›½
ğ‘–
â€‹
ğ±
ğ‘–
âˆ’
1
+
ğ›½
ğ‘–
â€‹
ğ³
ğ‘–
âˆ’
1
,
ğ‘–
=
1
,
â‹¯
,
ğ‘
.
(10)
As 
ğ‘
â†’
âˆ
, Eq. 10 converges to the following SDE,

d
â€‹
ğ±
=
âˆ’
1
2
â€‹
ğ›½
â€‹
(
ğ‘¡
)
â€‹
ğ±
â€‹
d
â€‹
ğ‘¡
+
ğ›½
â€‹
(
ğ‘¡
)
â€‹
d
â€‹
ğ°
.
(11)
Therefore, the noise perturbations used in SMLD and DDPM correspond to discretizations of SDEs Eqs. 9 and 11. Interestingly, the SDE of Eq. 9 always gives a process with exploding variance when 
ğ‘¡
â†’
âˆ
, whilst the SDE of Eq. 11 yields a process with a fixed variance of one when the initial distribution has unit variance (proof in Appendix B). Due to this difference, we hereafter refer to Eq. 9 as the Variance Exploding (VE) SDE, and Eq. 11 the Variance Preserving (VP) SDE.

Inspired by the VP SDE, we propose a new type of SDEs which perform particularly well on likelihoods (see Section 4.3), given by

d
â€‹
ğ±
=
âˆ’
1
2
â€‹
ğ›½
â€‹
(
ğ‘¡
)
â€‹
ğ±
â€‹
d
â€‹
ğ‘¡
+
ğ›½
â€‹
(
ğ‘¡
)
â€‹
(
1
âˆ’
ğ‘’
âˆ’
2
â€‹
âˆ«
0
ğ‘¡
ğ›½
â€‹
(
ğ‘ 
)
â€‹
d
ğ‘ 
)
â€‹
d
â€‹
ğ°
.
(12)
When using the same 
ğ›½
â€‹
(
ğ‘¡
)
 and starting from the same initial distribution, the variance of the stochastic process induced by Eq. 12 is always bounded by the VP SDE at every intermediate time step (proof in Appendix B). For this reason, we name Eq. 12 the sub-VP SDE.

Since VE, VP and sub-VP SDEs all have affine drift coefficients, their perturbation kernels 
ğ‘
0
â€‹
ğ‘¡
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
âˆ£
ğ±
â€‹
(
0
)
)
 are all Gaussian and can be computed in closed-forms, as discussed in Section 3.3. This makes training with Eq. 7 particularly efficient.

4Solving the reverse SDE
After training a time-dependent score-based model 
ğ¬
ğœ½
, we can use it to construct the reverse-time SDE and then simulate it with numerical approaches to generate samples from 
ğ‘
0
.

4.1General-purpose numerical SDE solvers
Numerical solvers provide approximate trajectories from SDEs. Many general-purpose numerical methods exist for solving SDEs, such as Euler-Maruyama and stochastic Runge-Kutta methods (Kloeden & Platen, 2013), which correspond to different discretizations of the stochastic dynamics. We can apply any of them to the reverse-time SDE for sample generation.

Ancestral sampling, the sampling method of DDPM (Eq. 4), actually corresponds to one special discretization of the reverse-time VP SDE (Eq. 11) (see Appendix E). Deriving the ancestral sampling rules for new SDEs, however, can be non-trivial. To remedy this, we propose reverse diffusion samplers (details in Appendix E), which discretize the reverse-time SDE in the same way as the forward one, and thus can be readily derived given the forward discretization. As shown in Table 1, reverse diffusion samplers perform slightly better than ancestral sampling for both SMLD and DDPM models on CIFAR-10 (DDPM-type ancestral sampling is also applicable to SMLD models, see Appendix F.)

4.2Predictor-corrector samplers
Unlike generic SDEs, we have additional information that can be used to improve solutions. Since we have a score-based model 
ğ¬
ğœ½
âˆ—
â€‹
(
ğ±
,
ğ‘¡
)
â‰ˆ
âˆ‡
ğ±
log
â¡
ğ‘
ğ‘¡
â€‹
(
ğ±
)
, we can employ score-based MCMC approaches, such as Langevin MCMC (Parisi, 1981; Grenander & Miller, 1994) or HMC (Neal et al., 2011) to sample from 
ğ‘
ğ‘¡
 directly, and correct the solution of a numerical SDE solver.

Specifically, at each time step, the numerical SDE solver first gives an estimate of the sample at the next time step, playing the role of a â€œpredictorâ€. Then, the score-based MCMC approach corrects the marginal distribution of the estimated sample, playing the role of a â€œcorrectorâ€. The idea is analogous to Predictor-Corrector methods, a family of numerical continuation techniques for solving systems of equations (Allgower & Georg, 2012), and we similarly name our hybrid sampling algorithms Predictor-Corrector (PC) samplers. Please find pseudo-code and a complete description in Appendix G. PC samplers generalize the original sampling methods of SMLD and DDPM: the former uses an identity function as the predictor and annealed Langevin dynamics as the corrector, while the latter uses ancestral sampling as the predictor and identity as the corrector.

Table 1:Comparing different reverse-time SDE solvers on CIFAR-10. Shaded regions are obtained with the same computation (number of score function evaluations). Mean and standard deviation are reported over five sampling runs. â€œP1000â€ or â€œP2000â€: predictor-only samplers using 1000 or 2000 steps. â€œC2000â€: corrector-only samplers using 2000 steps. â€œPC1000â€: Predictor-Corrector (PC) samplers using 1000 predictor and 1000 corrector steps.
  \bigstrut	Variance Exploding SDE (SMLD)	Variance Preserving SDE (DDPM)
 \bigstrut
Predictor
Sampler
FID
â†“
P1000	P2000	C2000	PC1000	P1000	P2000	C2000	PC1000
 \bigstrutancestral sampling	4.98 
Â±
 .06
4.88 
Â±
 .06
3.62 
Â±
 .03
3.24 
Â±
 .02
3.24 
Â±
 .02
3.21 
Â±
 .02
reverse diffusion	4.79 
Â±
 .07
4.74 
Â±
 .08
3.60 
Â±
 .02
3.21 
Â±
 .02
3.19 
Â±
 .02
3.18 
Â±
 .01
probability flow	15.41 
Â±
 .15
10.54 
Â±
 .08
20.43 
Â±
 .07
3.51 
Â±
 .04
3.59 
Â±
 .04
3.23 
Â±
 .03
19.06 
Â±
 .06
3.06 
Â±
 .03
 								

We test PC samplers on SMLD and DDPM models (see Algorithms 2 and 3 in Appendix G) trained with original discrete objectives given by Eqs. 1 and 3. This exhibits the compatibility of PC samplers to score-based models trained with a fixed number of noise scales. We summarize the performance of different samplers in Table 1, where probability flow is a predictor to be discussed in Section 4.3. Detailed experimental settings and additional results are given in Appendix G. We observe that our reverse diffusion sampler always outperform ancestral sampling, and corrector-only methods (C2000) perform worse than other competitors (P2000, PC1000) with the same computation (In fact, we need way more corrector steps per noise scale, and thus more computation, to match the performance of other samplers.) For all predictors, adding one corrector step for each predictor step (PC1000) doubles computation but always improves sample quality (against P1000). Moreover, it is typically better than doubling the number of predictor steps without adding a corrector (P2000), where we have to interpolate between noise scales in an ad hoc manner (detailed in Appendix G) for SMLD/DDPM models. In Fig. 9 (Appendix G), we additionally provide qualitative comparison for models trained with the continuous objective Eq. 7 on 
256
Ã—
256
 LSUN images and the VE SDE, where PC samplers clearly surpass predictor-only samplers under comparable computation, when using a proper number of corrector steps.

4.3Probability flow and connection to neural ODEs
Score-based models enable another numerical method for solving the reverse-time SDE. For all diffusion processes, there exists a corresponding deterministic process whose trajectories share the same marginal probability densities 
{
ğ‘
ğ‘¡
â€‹
(
ğ±
)
}
ğ‘¡
=
0
ğ‘‡
 as the SDE. This deterministic process satisfies an ODE (more details in Section D.1):

d
â€‹
ğ±
=
[
ğŸ
â€‹
(
ğ±
,
ğ‘¡
)
âˆ’
1
2
â€‹
ğ‘”
â€‹
(
ğ‘¡
)
2
â€‹
âˆ‡
ğ±
log
â¡
ğ‘
ğ‘¡
â€‹
(
ğ±
)
]
â€‹
d
â€‹
ğ‘¡
,
(13)
which can be determined from the SDE once scores are known. We name the ODE in Eq. 13 the probability flow ODE. When the score function is approximated by the time-dependent score-based model, which is typically a neural network, this is an example of a neural ODE (Chen et al., 2018).

Exact likelihood computation  Leveraging the connection to neural ODEs, we can compute the density defined by Eq. 13 via the instantaneous change of variables formula (Chen et al., 2018). This allows us to compute the exact likelihood on any input data (details in Section D.2). As an example, we report negative log-likelihoods (NLLs) measured in bits/dim on the CIFAR-10 dataset in Table 2. We compute log-likelihoods on uniformly dequantized data, and only compare to models evaluated in the same way (omitting models evaluated with variational dequantization (Ho et al., 2019) or discrete data), except for DDPM (
ğ¿
/
ğ¿
simple
) whose ELBO values (annotated with *) are reported on discrete data. Main results: (i) For the same DDPM model in Ho et al. (2020), we obtain better bits/dim than ELBO, since our likelihoods are exact; (ii) Using the same architecture, we trained another DDPM model with the continuous objective in Eq. 7 (i.e., DDPM cont.), which further improves the likelihood; (iii) With sub-VP SDEs, we always get higher likelihoods compared to VP SDEs; (iv) With improved architecture (i.e., DDPM++ cont., details in Section 4.4) and the sub-VP SDE, we can set a new record bits/dim of 2.99 on uniformly dequantized CIFAR-10 even without maximum likelihood training.

Table 2:NLLs and FIDs (ODE) on CIFAR-10.
Model	NLL Test 
â†“
FID 
â†“
RealNVP (Dinh et al., 2016)	3.49	-
iResNet (Behrmann et al., 2019)	3.45	-
Glow (Kingma & Dhariwal, 2018)	3.35	-
MintNet (Song et al., 2019b)	3.32	-
Residual Flow (Chen et al., 2019)	3.28	46.37
FFJORD (Grathwohl et al., 2018)	3.40	-
Flow++ (Ho et al., 2019)	3.29	-
DDPM (
ğ¿
) (Ho et al., 2020)	
â‰¤
 3.70*	13.51
DDPM (
ğ¿
simple
) (Ho et al., 2020)	
â‰¤
 3.75*	3.17
DDPM	3.28	3.37
DDPM cont. (VP)	3.21	3.69
DDPM cont. (sub-VP)	3.05	3.56
DDPM++ cont. (VP)	3.16	3.93
DDPM++ cont. (sub-VP)	3.02	3.16
DDPM++ cont. (deep, VP)	3.13	3.08
DDPM++ cont. (deep, sub-VP)	2.99	2.92

Table 3:CIFAR-10 sample quality.
Model	FID
â†“
IS
â†‘
Conditional		
BigGAN (Brock et al., 2018)	14.73	9.22
StyleGAN2-ADA (Karras et al., 2020a)	2.42	10.14
Unconditional		
StyleGAN2-ADA (Karras et al., 2020a)	2.92	9.83
NCSN (Song & Ermon, 2019)	25.32	8.87 
Â±
 .12
NCSNv2 (Song & Ermon, 2020)	10.87	8.40 
Â±
 .07
DDPM (Ho et al., 2020)	3.17	9.46 
Â±
 .11
DDPM++	2.78	9.64
DDPM++ cont. (VP)	2.55	9.58
DDPM++ cont. (sub-VP)	2.61	9.56
DDPM++ cont. (deep, VP)	2.41	9.68
DDPM++ cont. (deep, sub-VP)	2.41	9.57
NCSN++	2.45	9.73
NCSN++ cont. (VE)	2.38	9.83
NCSN++ cont. (deep, VE)	2.20	9.89

Manipulating latent representations  By integrating Eq. 13, we can encode any datapoint 
ğ±
â€‹
(
0
)
 into a latent space 
ğ±
â€‹
(
ğ‘‡
)
. Decoding can be achieved by integrating a corresponding ODE for the reverse-time SDE. As is done with other invertible models such as neural ODEs and normalizing flows (Dinh et al., 2016; Kingma & Dhariwal, 2018), we can manipulate this latent representation for image editing, such as interpolation, and temperature scaling (see Fig. 3 and Section D.4).

Uniquely identifiable encoding  Unlike most current invertible models, our encoding is uniquely identifiable, meaning that with sufficient training data, model capacity, and optimization accuracy, the encoding for an input is uniquely determined by the data distribution (Roeder et al., 2020). This is because our forward SDE, Eq. 5, has no trainable parameters, and its associated probability flow ODE, Eq. 13, provides the same trajectories given perfectly estimated scores. We provide additional empirical verification on this property in Section D.5.

Efficient sampling  As with neural ODEs, we can sample 
ğ±
â€‹
(
0
)
âˆ¼
ğ‘
0
 by solving Eq. 13 from different final conditions 
ğ±
â€‹
(
ğ‘‡
)
âˆ¼
ğ‘
ğ‘‡
. Using a fixed discretization strategy we can generate competitive samples, especially when used in conjuction with correctors (Table 1, â€œprobability flow samplerâ€, details in Section D.3). Using a black-box ODE solver (Dormand & Prince, 1980) not only produces high quality samples (Table 2, details in Section D.4), but also allows us to explicitly trade-off accuracy for efficiency. With a larger error tolerance, the number of function evaluations can be reduced by over 
90
%
 without affecting the visual quality of samples (Fig. 3).

Refer to caption
Figure 3:Probability flow ODE enables fast sampling with adaptive stepsizes as the numerical precision is varied (left), and reduces the number of score function evaluations (NFE) without harming quality (middle). The invertible mapping from latents to images allows for interpolations (right).
4.4Architecture improvements
We explore several new architecture designs for score-based models using both VE and VP SDEs (details in Appendix H), where we train models with the same discrete objectives as in SMLD/DDPM. We directly transfer the architectures for VP SDEs to sub-VP SDEs due to their similarity. Our optimal architecture for the VE SDE, named NCSN++, achieves an FID of 2.45 on CIFAR-10 with PC samplers, while our optimal architecture for the VP SDE, called DDPM++, achieves 2.78.

By switching to the continuous training objective in Eq. 7, and increasing the network depth, we can further improve sample quality for all models. The resulting architectures are denoted as NCSN++ cont. and DDPM++ cont. in Table 3 for VE and VP/sub-VP SDEs respectively. Results reported in Table 3 are for the checkpoint with the smallest FID over the course of training, where samples are generated with PC samplers. In contrast, FID scores and NLL values in Table 2 are reported for the last training checkpoint, and samples are obtained with black-box ODE solvers. As shown in Table 3, VE SDEs typically provide better sample quality than VP/sub-VP SDEs, but we also empirically observe that their likelihoods are worse than VP/sub-VP SDE counterparts. This indicates that practitioners likely need to experiment with different SDEs for varying domains and architectures.

Our best model for sample quality, NCSN++ cont. (deep, VE), doubles the network depth and sets new records for both inception score and FID on unconditional generation for CIFAR-10. Surprisingly, we can achieve better FID than the previous best conditional generative model without requiring labeled data. With all improvements together, we also obtain the first set of high-fidelity samples on CelebA-HQ 
1024
Ã—
1024
 from score-based models (see Section H.3). Our best model for likelihoods, DDPM++ cont. (deep, sub-VP), similarly doubles the network depth and achieves a log-likelihood of 2.99 bits/dim with the continuous objective in Eq. 7. To our best knowledge, this is the highest likelihood on uniformly dequantized CIFAR-10.

5Controllable generation
The continuous structure of our framework allows us to not only produce data samples from 
ğ‘
0
, but also from 
ğ‘
0
â€‹
(
ğ±
â€‹
(
0
)
âˆ£
ğ²
)
 if 
ğ‘
ğ‘¡
â€‹
(
ğ²
âˆ£
ğ±
â€‹
(
ğ‘¡
)
)
 is known. Given a forward SDE as in Eq. 5, we can sample from 
ğ‘
ğ‘¡
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
âˆ£
ğ²
)
 by starting from 
ğ‘
ğ‘‡
â€‹
(
ğ±
â€‹
(
ğ‘‡
)
âˆ£
ğ²
)
 and solving a conditional reverse-time SDE:

d
â€‹
ğ±
=
{
ğŸ
â€‹
(
ğ±
,
ğ‘¡
)
âˆ’
ğ‘”
â€‹
(
ğ‘¡
)
2
â€‹
[
âˆ‡
ğ±
log
â¡
ğ‘
ğ‘¡
â€‹
(
ğ±
)
+
âˆ‡
ğ±
log
â¡
ğ‘
ğ‘¡
â€‹
(
ğ²
âˆ£
ğ±
)
]
}
â€‹
d
â€‹
ğ‘¡
+
ğ‘”
â€‹
(
ğ‘¡
)
â€‹
d
â€‹
ğ°
Â¯
.
(14)
In general, we can use Eq. 14 to solve a large family of inverse problems with score-based generative models, once given an estimate of the gradient of the forward process, 
âˆ‡
ğ±
log
â¡
ğ‘
ğ‘¡
â€‹
(
ğ²
âˆ£
ğ±
â€‹
(
ğ‘¡
)
)
. In some cases, it is possible to train a separate model to learn the forward process 
log
â¡
ğ‘
ğ‘¡
â€‹
(
ğ²
âˆ£
ğ±
â€‹
(
ğ‘¡
)
)
 and compute its gradient. Otherwise, we may estimate the gradient with heuristics and domain knowledge. In Section I.4, we provide a broadly applicable method for obtaining such an estimate without the need of training auxiliary models.

We consider three applications of controllable generation with this approach: class-conditional generation, image imputation and colorization. When 
ğ²
 represents class labels, we can train a time-dependent classifier 
ğ‘
ğ‘¡
â€‹
(
ğ²
âˆ£
ğ±
â€‹
(
ğ‘¡
)
)
 for class-conditional sampling. Since the forward SDE is tractable, we can easily create training data 
(
ğ±
â€‹
(
ğ‘¡
)
,
ğ²
)
 for the time-dependent classifier by first sampling 
(
ğ±
â€‹
(
0
)
,
ğ²
)
 from a dataset, and then sampling 
ğ±
â€‹
(
ğ‘¡
)
âˆ¼
ğ‘
0
â€‹
ğ‘¡
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
âˆ£
ğ±
â€‹
(
0
)
)
. Afterwards, we may employ a mixture of cross-entropy losses over different time steps, like Eq. 7, to train the time-dependent classifier 
ğ‘
ğ‘¡
â€‹
(
ğ²
âˆ£
ğ±
â€‹
(
ğ‘¡
)
)
. We provide class-conditional CIFAR-10 samples in Fig. 4 (left), and relegate more details and results to Appendix I.

Imputation is a special case of conditional sampling. Suppose we have an incomplete data point 
ğ²
 where only some subset, 
Î©
â€‹
(
ğ²
)
 is known. Imputation amounts to sampling from 
ğ‘
â€‹
(
ğ±
â€‹
(
0
)
âˆ£
Î©
â€‹
(
ğ²
)
)
, which we can accomplish using an unconditional model (see Section I.2). Colorization is a special case of imputation, except that the known data dimensions are coupled. We can decouple these data dimensions with an orthogonal linear transformation, and perform imputation in the transformed space (details in Section I.3). Fig. 4 (right) shows results for inpainting and colorization achieved with unconditional time-dependent score-based models.

Refer to caption
Refer to caption
Figure 4:Left: Class-conditional samples on 
32
Ã—
32
 CIFAR-10. Top four rows are automobiles and bottom four rows are horses. Right: Inpainting (top two rows) and colorization (bottom two rows) results on 
256
Ã—
256
 LSUN. First column is the original image, second column is the masked/gray-scale image, remaining columns are sampled image completions or colorizations.
6Conclusion
We presented a framework for score-based generative modeling based on SDEs. Our work enables a better understanding of existing approaches, new sampling algorithms, exact likelihood computation, uniquely identifiable encoding, latent code manipulation, and brings new conditional generation abilities to the family of score-based generative models.

While our proposed sampling approaches improve results and enable more efficient sampling, they remain slower at sampling than GANs (Goodfellow et al., 2014) on the same datasets. Identifying ways of combining the stable learning of score-based generative models with the fast sampling of implicit models like GANs remains an important research direction. Additionally, the breadth of samplers one can use when given access to score functions introduces a number of hyper-parameters. Future work would benefit from improved methods to automatically select and tune these hyper-parameters, as well as more extensive investigation on the merits and limitations of various samplers.

Acknowledgements
We would like to thank Nanxin Chen, Ruiqi Gao, Jonathan Ho, Kevin Murphy, Tim Salimans and Han Zhang for their insightful discussions during the course of this project. This research was partially supported by NSF (#1651565, #1522054, #1733686), ONR (N000141912145), AFOSR (FA95501910024), and TensorFlow Research Cloud. Yang Song was partially supported by the Apple PhD Fellowship in AI/ML.

References
Allgower & Georg (2012)Eugene L Allgower and Kurt Georg.Numerical continuation methods: an introduction, volume 13.Springer Science & Business Media, 2012.
Anderson (1982)Brian D O Anderson.Reverse-time diffusion equation models.Stochastic Process. Appl., 12(3):313â€“326, May 1982.
Behrmann et al. (2019)Jens Behrmann, Will Grathwohl, Ricky TQ Chen, David Duvenaud, and JÃ¶rn-Henrik Jacobsen.Invertible residual networks.In International Conference on Machine Learning, pp. 573â€“582, 2019.
Bordes et al. (2017)Florian Bordes, Sina Honari, and Pascal Vincent.Learning to generate samples from noise through infusion training.arXiv preprint arXiv:1703.06975, 2017.
Brock et al. (2018)Andrew Brock, Jeff Donahue, and Karen Simonyan.Large scale gan training for high fidelity natural image synthesis.In International Conference on Learning Representations, 2018.
Cai et al. (2020)Ruojin Cai, Guandao Yang, Hadar Averbuch-Elor, Zekun Hao, Serge Belongie, Noah Snavely, and Bharath Hariharan.Learning gradient fields for shape generation.In Proceedings of the European Conference on Computer Vision (ECCV), 2020.
Chen et al. (2020)Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, and William Chan.Wavegrad: Estimating gradients for waveform generation.arXiv preprint arXiv:2009.00713, 2020.
Chen et al. (2018)Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud.Neural ordinary differential equations.In Advances in neural information processing systems, pp. 6571â€“6583, 2018.
Chen et al. (2019)Ricky TQ Chen, Jens Behrmann, David K Duvenaud, and JÃ¶rn-Henrik Jacobsen.Residual flows for invertible generative modeling.In Advances in Neural Information Processing Systems, pp. 9916â€“9926, 2019.
Dinh et al. (2016)Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio.Density estimation using real nvp.arXiv preprint arXiv:1605.08803, 2016.
Dormand & Prince (1980)John R Dormand and Peter J Prince.A family of embedded runge-kutta formulae.Journal of computational and applied mathematics, 6(1):19â€“26, 1980.
Du & Mordatch (2019)Yilun Du and Igor Mordatch.Implicit generation and modeling with energy based models.In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'AlchÃ©-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32, pp.  3608â€“3618. Curran Associates, Inc., 2019.
Efron (2011)Bradley Efron.Tweedieâ€™s formula and selection bias.Journal of the American Statistical Association, 106(496):1602â€“1614, 2011.
Goodfellow et al. (2014)Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.Generative adversarial nets.In Advances in neural information processing systems, pp. 2672â€“2680, 2014.
Goyal et al. (2017)Anirudh Goyal Alias Parth Goyal, Nan Rosemary Ke, Surya Ganguli, and Yoshua Bengio.Variational walkback: Learning a transition operator as a stochastic recurrent net.In Advances in Neural Information Processing Systems, pp. 4392â€“4402, 2017.
Grathwohl et al. (2018)Will Grathwohl, Ricky TQ Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud.Ffjord: Free-form continuous dynamics for scalable reversible generative models.In International Conference on Learning Representations, 2018.
Grenander & Miller (1994)Ulf Grenander and Michael I Miller.Representations of knowledge in complex systems.Journal of the Royal Statistical Society: Series B (Methodological), 56(4):549â€“581, 1994.
Ho et al. (2019)Jonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel.Flow++: Improving flow-based generative models with variational dequantization and architecture design.In International Conference on Machine Learning, pp. 2722â€“2730, 2019.
Ho et al. (2020)Jonathan Ho, Ajay Jain, and Pieter Abbeel.Denoising diffusion probabilistic models.Advances in Neural Information Processing Systems, 33, 2020.
Hutchinson (1990)Michael F Hutchinson.A stochastic estimator of the trace of the influence matrix for Laplacian smoothing splines.Communications in Statistics-Simulation and Computation, 19(2):433â€“450, 1990.
HyvÃ¤rinen (2005)Aapo HyvÃ¤rinen.Estimation of non-normalized statistical models by score matching.Journal of Machine Learning Research, 6(Apr):695â€“709, 2005.
Jolicoeur-Martineau et al. (2020)Alexia Jolicoeur-Martineau, RÃ©mi PichÃ©-Taillefer, RÃ©mi Tachet des Combes, and Ioannis Mitliagkas.Adversarial score matching and improved sampling for image generation.arXiv preprint arXiv:2009.05475, 2020.
Karras et al. (2018)Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.Progressive growing of gans for improved quality, stability, and variation.In International Conference on Learning Representations, 2018.
Karras et al. (2019)Tero Karras, Samuli Laine, and Timo Aila.A style-based generator architecture for generative adversarial networks.In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.  4401â€“4410, 2019.
Karras et al. (2020a)Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila.Training generative adversarial networks with limited data.Advances in Neural Information Processing Systems, 33, 2020a.
Karras et al. (2020b)Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila.Analyzing and improving the image quality of StyleGAN.In Proc. CVPR, 2020b.
Kingma & Dhariwal (2018)Durk P Kingma and Prafulla Dhariwal.Glow: Generative flow with invertible 1x1 convolutions.In Advances in Neural Information Processing Systems, pp. 10215â€“10224, 2018.
Kloeden & Platen (2013)Peter E Kloeden and Eckhard Platen.Numerical solution of stochastic differential equations, volume 23.Springer Science & Business Media, 2013.
Kong et al. (2020)Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro.Diffwave: A versatile diffusion model for audio synthesis.arXiv preprint arXiv:2009.09761, 2020.
Krizhevsky et al. (2009)Alex Krizhevsky, Geoffrey Hinton, et al.Learning multiple layers of features from tiny images.2009.
Liu et al. (2015)Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.Deep learning face attributes in the wild.In Proceedings of International Conference on Computer Vision (ICCV), December 2015.
Maoutsa et al. (2020)Dimitra Maoutsa, Sebastian Reich, and Manfred Opper.Interacting particle solutions of fokker-planck equations through gradient-log-density estimation.arXiv preprint arXiv:2006.00702, 2020.
Neal et al. (2011)Radford M Neal et al.Mcmc using hamiltonian dynamics.Handbook of markov chain monte carlo, 2(11):2, 2011.
Niu et al. (2020)Chenhao Niu, Yang Song, Jiaming Song, Shengjia Zhao, Aditya Grover, and Stefano Ermon.Permutation invariant graph generation via score-based generative modeling.volume 108 of Proceedings of Machine Learning Research, pp. 4474â€“4484, Online, 26â€“28 Aug 2020. PMLR.
Ã˜ksendal (2003)Bernt Ã˜ksendal.Stochastic differential equations.In Stochastic differential equations, pp.  65â€“84. Springer, 2003.
Pang et al. (2020)Tianyu Pang, Kun Xu, Chongxuan Li, Yang Song, Stefano Ermon, and Jun Zhu.Efficient learning of generative models via finite-difference score matching.arXiv preprint arXiv:2007.03317, 2020.
Parisi (1981)Giorgio Parisi.Correlation functions and computer simulations.Nuclear Physics B, 180(3):378â€“384, 1981.
Razavi et al. (2019)Ali Razavi, Aaron van den Oord, and Oriol Vinyals.Generating diverse high-fidelity images with vq-vae-2.In Advances in Neural Information Processing Systems, pp. 14837â€“14847, 2019.
Roeder et al. (2020)Geoffrey Roeder, Luke Metz, and Diederik P Kingma.On linear identifiability of learned representations.arXiv preprint arXiv:2007.00810, 2020.
SÃ¤rkkÃ¤ & Solin (2019)Simo SÃ¤rkkÃ¤ and Arno Solin.Applied stochastic differential equations, volume 10.Cambridge University Press, 2019.
Skilling (1989)John Skilling.The eigenvalues of mega-dimensional matrices.In Maximum Entropy and Bayesian Methods, pp.  455â€“466. Springer, 1989.
Sohl-Dickstein et al. (2015)Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli.Deep unsupervised learning using nonequilibrium thermodynamics.In International Conference on Machine Learning, pp. 2256â€“2265, 2015.
Song & Ermon (2019)Yang Song and Stefano Ermon.Generative modeling by estimating gradients of the data distribution.In Advances in Neural Information Processing Systems, pp. 11895â€“11907, 2019.
Song & Ermon (2020)Yang Song and Stefano Ermon.Improved techniques for training score-based generative models.Advances in Neural Information Processing Systems, 33, 2020.
Song et al. (2019a)Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon.Sliced score matching: A scalable approach to density and score estimation.In Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI 2019, Tel Aviv, Israel, July 22-25, 2019, pp.  204, 2019a.
Song et al. (2019b)Yang Song, Chenlin Meng, and Stefano Ermon.Mintnet: Building invertible neural networks with masked convolutions.In Advances in Neural Information Processing Systems, pp. 11002â€“11012, 2019b.
Tancik et al. (2020)Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan T. Barron, and Ren Ng.Fourier features let networks learn high frequency functions in low dimensional domains.NeurIPS, 2020.
Vincent (2011)Pascal Vincent.A connection between score matching and denoising autoencoders.Neural computation, 23(7):1661â€“1674, 2011.
Yu et al. (2015)Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao.Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop.arXiv preprint arXiv:1506.03365, 2015.
Zagoruyko & Komodakis (2016)Sergey Zagoruyko and Nikos Komodakis.Wide residual networks.arXiv preprint arXiv:1605.07146, 2016.
Zhang (2019)Richard Zhang.Making convolutional networks shift-invariant again.In ICML, 2019.
Appendix
We include several appendices with additional details, derivations, and results. Our framework allows general SDEs with matrix-valued diffusion coefficients that depend on the state, for which we provide a detailed discussion in Appendix A. We give a full derivation of VE, VP and sub-VP SDEs in Appendix B, and discuss how to use them from a practitionerâ€™s perspective in Appendix C. We elaborate on the probability flow formulation of our framework in Appendix D, including a derivation of the probability flow ODE (Section D.1), exact likelihood computation (Section D.2), probability flow sampling with a fixed discretization strategy (Section D.3), sampling with black-box ODE solvers (Section D.4), and experimental verification on uniquely identifiable encoding (Section D.5). We give a full description of the reverse diffusion sampler in Appendix E, the DDPM-type ancestral sampler for SMLD models in Appendix F, and Predictor-Corrector samplers in Appendix G. We explain our model architectures and detailed experimental settings in Appendix H, with 
1024
Ã—
1024
 CelebA-HQ samples therein. Finally, we detail on the algorithms for controllable generation in Appendix I, and include extended results for class-conditional generation (Section I.1), image inpainting (Section I.2), colorization (Section I.3), and a strategy for solving general inverse problems (Section I.4).

Appendix AThe framework for more general SDEs
In the main text, we introduced our framework based on a simplified SDE Eq. 5 where the diffusion coefficient is independent of 
ğ±
â€‹
(
ğ‘¡
)
. It turns out that our framework can be extended to hold for more general diffusion coefficients. We can consider SDEs in the following form:

d
â€‹
ğ±
=
ğŸ
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
d
â€‹
ğ‘¡
+
ğ†
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
d
â€‹
ğ°
,
(15)
where 
ğŸ
â€‹
(
â‹…
,
ğ‘¡
)
:
â„
ğ‘‘
â†’
â„
ğ‘‘
 and 
ğ†
â€‹
(
â‹…
,
ğ‘¡
)
:
â„
ğ‘‘
â†’
â„
ğ‘‘
Ã—
ğ‘‘
. We follow the ItÃ´ interpretation of SDEs throughout this paper.

According to (Anderson, 1982), the reverse-time SDE is given by (cf., Eq. 6)

d
â€‹
ğ±
=
{
ğŸ
â€‹
(
ğ±
,
ğ‘¡
)
âˆ’
âˆ‡
â‹…
[
ğ†
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
ğ†
â€‹
(
ğ±
,
ğ‘¡
)
ğ–³
]
âˆ’
ğ†
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
ğ†
â€‹
(
ğ±
,
ğ‘¡
)
ğ–³
â€‹
âˆ‡
ğ±
log
â¡
ğ‘
ğ‘¡
â€‹
(
ğ±
)
}
â€‹
d
â€‹
ğ‘¡
+
ğ†
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
d
â€‹
ğ°
Â¯
,
(16)
where we define 
âˆ‡
â‹…
ğ…
â€‹
(
ğ±
)
:=
(
âˆ‡
â‹…
ğŸ
1
â€‹
(
ğ±
)
,
âˆ‡
â‹…
ğŸ
2
â€‹
(
ğ±
)
,
â‹¯
,
âˆ‡
â‹…
ğŸ
ğ‘‘
â€‹
(
ğ±
)
)
ğ–³
 for a matrix-valued function 
ğ…
â€‹
(
ğ±
)
:=
(
ğŸ
1
â€‹
(
ğ±
)
,
ğŸ
2
â€‹
(
ğ±
)
,
â‹¯
,
ğŸ
ğ‘‘
â€‹
(
ğ±
)
)
ğ–³
 throughout the paper.

The probability flow ODE corresponding to Eq. 15 has the following form (cf., Eq. 13, see a detailed derivation in Section D.1):

d
â€‹
ğ±
=
{
ğŸ
â€‹
(
ğ±
,
ğ‘¡
)
âˆ’
1
2
â€‹
âˆ‡
â‹…
[
ğ†
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
ğ†
â€‹
(
ğ±
,
ğ‘¡
)
ğ–³
]
âˆ’
1
2
â€‹
ğ†
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
ğ†
â€‹
(
ğ±
,
ğ‘¡
)
ğ–³
â€‹
âˆ‡
ğ±
log
â¡
ğ‘
ğ‘¡
â€‹
(
ğ±
)
}
â€‹
d
â€‹
ğ‘¡
.
(17)
Finally for conditional generation with the general SDE Eq. 15, we can solve the conditional reverse-time SDE below (cf., Eq. 14, details in Appendix I):

d
ğ±
=
{
ğŸ
(
ğ±
,
ğ‘¡
)
âˆ’
âˆ‡
â‹…
[
ğ†
(
ğ±
,
ğ‘¡
)
ğ†
(
ğ±
,
ğ‘¡
)
ğ–³
]
âˆ’
ğ†
(
ğ±
,
ğ‘¡
)
ğ†
(
ğ±
,
ğ‘¡
)
ğ–³
âˆ‡
ğ±
log
ğ‘
ğ‘¡
(
ğ±
)
âˆ’
ğ†
(
ğ±
,
ğ‘¡
)
ğ†
(
ğ±
,
ğ‘¡
)
ğ–³
âˆ‡
ğ±
log
ğ‘
ğ‘¡
(
ğ²
âˆ£
ğ±
)
}
d
ğ‘¡
+
ğ†
(
ğ±
,
ğ‘¡
)
d
ğ°
Â¯
.
(18)
When the drift and diffusion coefficient of an SDE are not affine, it can be difficult to compute the transition kernel 
ğ‘
0
â€‹
ğ‘¡
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
âˆ£
ğ±
â€‹
(
0
)
)
 in closed form. This hinders the training of score-based models, because Eq. 7 requires knowing 
âˆ‡
ğ±
â€‹
(
ğ‘¡
)
log
â¡
ğ‘
0
â€‹
ğ‘¡
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
âˆ£
ğ±
â€‹
(
0
)
)
. To overcome this difficulty, we can replace denoising score matching in Eq. 7 with other efficient variants of score matching that do not require computing 
âˆ‡
ğ±
â€‹
(
ğ‘¡
)
log
â¡
ğ‘
0
â€‹
ğ‘¡
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
âˆ£
ğ±
â€‹
(
0
)
)
. For example, when using sliced score matching (Song et al., 2019a), our training objective Eq. 7 becomes

ğœ½
âˆ—
=
arg
â€‹
min
ğœ½
â¡
ğ”¼
ğ‘¡
â€‹
{
ğœ†
â€‹
(
ğ‘¡
)
â€‹
ğ”¼
ğ±
â€‹
(
0
)
â€‹
ğ”¼
ğ±
â€‹
(
ğ‘¡
)
â€‹
ğ”¼
ğ¯
âˆ¼
ğ‘
ğ¯
â€‹
[
1
2
â€‹
âˆ¥
ğ¬
ğœ½
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
,
ğ‘¡
)
âˆ¥
2
2
+
ğ¯
ğ–³
â€‹
ğ¬
ğœ½
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
,
ğ‘¡
)
â€‹
ğ¯
]
}
,
(19)
where 
ğœ†
:
[
0
,
ğ‘‡
]
â†’
â„
+
 is a positive weighting function, 
ğ‘¡
âˆ¼
ğ’°
â€‹
(
0
,
ğ‘‡
)
, 
ğ”¼
â€‹
[
ğ¯
]
=
ğŸ
, and 
Cov
â¡
[
ğ¯
]
=
ğˆ
. We can always simulate the SDE to sample from 
ğ‘
0
â€‹
ğ‘¡
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
âˆ£
ğ±
â€‹
(
0
)
)
, and solve Eq. 19 to train the time-dependent score-based model 
ğ¬
ğœ½
â€‹
(
ğ±
,
ğ‘¡
)
.

Appendix BVE, VP and sub-VP SDEs
Below we provide detailed derivations to show that the noise perturbations of SMLD and DDPM are discretizations of the Variance Exploding (VE) and Variance Preserving (VP) SDEs respectively. We additionally introduce sub-VP SDEs, a modification to VP SDEs that often achieves better performance in both sample quality and likelihoods.

First, when using a total of 
ğ‘
 noise scales, each perturbation kernel 
ğ‘
ğœ
ğ‘–
â€‹
(
ğ±
âˆ£
ğ±
0
)
 of SMLD can be derived from the following Markov chain:

ğ±
ğ‘–
=
ğ±
ğ‘–
âˆ’
1
+
ğœ
ğ‘–
2
âˆ’
ğœ
ğ‘–
âˆ’
1
2
â€‹
ğ³
ğ‘–
âˆ’
1
,
ğ‘–
=
1
,
â‹¯
,
ğ‘
,
(20)
where 
ğ³
ğ‘–
âˆ’
1
âˆ¼
ğ’©
â€‹
(
ğŸ
,
ğˆ
)
, 
ğ±
0
âˆ¼
ğ‘
data
, and we have introduced 
ğœ
0
=
0
 to simplify the notation. In the limit of 
ğ‘
â†’
âˆ
, the Markov chain 
{
ğ±
ğ‘–
}
ğ‘–
=
1
ğ‘
 becomes a continuous stochastic process 
{
ğ±
â€‹
(
ğ‘¡
)
}
ğ‘¡
=
0
1
, 
{
ğœ
ğ‘–
}
ğ‘–
=
1
ğ‘
 becomes a function 
ğœ
â€‹
(
ğ‘¡
)
, and 
ğ³
ğ‘–
 becomes 
ğ³
â€‹
(
ğ‘¡
)
, where we have used a continuous time variable 
ğ‘¡
âˆˆ
[
0
,
1
]
 for indexing, rather than an integer 
ğ‘–
âˆˆ
{
1
,
2
,
â‹¯
,
ğ‘
}
. Let 
ğ±
â€‹
(
ğ‘–
ğ‘
)
=
ğ±
ğ‘–
, 
ğœ
â€‹
(
ğ‘–
ğ‘
)
=
ğœ
ğ‘–
, and 
ğ³
â€‹
(
ğ‘–
ğ‘
)
=
ğ³
ğ‘–
 for 
ğ‘–
=
1
,
2
,
â‹¯
,
ğ‘
. We can rewrite Eq. 20 as follows with 
Î”
â€‹
ğ‘¡
=
1
ğ‘
 and 
ğ‘¡
âˆˆ
{
0
,
1
ğ‘
,
â‹¯
,
ğ‘
âˆ’
1
ğ‘
}
:

ğ±
â€‹
(
ğ‘¡
+
Î”
â€‹
ğ‘¡
)
=
ğ±
â€‹
(
ğ‘¡
)
+
ğœ
2
â€‹
(
ğ‘¡
+
Î”
â€‹
ğ‘¡
)
âˆ’
ğœ
2
â€‹
(
ğ‘¡
)
â€‹
ğ³
â€‹
(
ğ‘¡
)
â‰ˆ
ğ±
â€‹
(
ğ‘¡
)
+
d
â€‹
[
ğœ
2
â€‹
(
ğ‘¡
)
]
d
â€‹
ğ‘¡
â€‹
Î”
â€‹
ğ‘¡
â€‹
ğ³
â€‹
(
ğ‘¡
)
,
where the approximate equality holds when 
Î”
â€‹
ğ‘¡
â‰ª
1
. In the limit of 
Î”
â€‹
ğ‘¡
â†’
0
, this converges to

d
â€‹
ğ±
=
d
â€‹
[
ğœ
2
â€‹
(
ğ‘¡
)
]
d
â€‹
ğ‘¡
â€‹
d
â€‹
ğ°
,
(21)
which is the VE SDE.

For the perturbation kernels 
{
ğ‘
ğ›¼
ğ‘–
â€‹
(
ğ±
âˆ£
ğ±
0
)
}
ğ‘–
=
1
ğ‘
 used in DDPM, the discrete Markov chain is

ğ±
ğ‘–
=
1
âˆ’
ğ›½
ğ‘–
â€‹
ğ±
ğ‘–
âˆ’
1
+
ğ›½
ğ‘–
â€‹
ğ³
ğ‘–
âˆ’
1
,
ğ‘–
=
1
,
â‹¯
,
ğ‘
,
(22)
where 
ğ³
ğ‘–
âˆ’
1
âˆ¼
ğ’©
â€‹
(
ğŸ
,
ğˆ
)
. To obtain the limit of this Markov chain when 
ğ‘
â†’
âˆ
, we define an auxiliary set of noise scales 
{
ğ›½
Â¯
ğ‘–
=
ğ‘
â€‹
ğ›½
ğ‘–
}
ğ‘–
=
1
ğ‘
, and re-write Eq. 22 as below

ğ±
ğ‘–
=
1
âˆ’
ğ›½
Â¯
ğ‘–
ğ‘
â€‹
ğ±
ğ‘–
âˆ’
1
+
ğ›½
Â¯
ğ‘–
ğ‘
â€‹
ğ³
ğ‘–
âˆ’
1
,
ğ‘–
=
1
,
â‹¯
,
ğ‘
.
(23)
In the limit of 
ğ‘
â†’
âˆ
, 
{
ğ›½
Â¯
ğ‘–
}
ğ‘–
=
1
ğ‘
 becomes a function 
ğ›½
â€‹
(
ğ‘¡
)
 indexed by 
ğ‘¡
âˆˆ
[
0
,
1
]
. Let 
ğ›½
â€‹
(
ğ‘–
ğ‘
)
=
ğ›½
Â¯
ğ‘–
, 
ğ±
â€‹
(
ğ‘–
ğ‘
)
=
ğ±
ğ‘–
, 
ğ³
â€‹
(
ğ‘–
ğ‘
)
=
ğ³
ğ‘–
. We can rewrite the Markov chain Eq. 23 as the following with 
Î”
â€‹
ğ‘¡
=
1
ğ‘
 and 
ğ‘¡
âˆˆ
{
0
,
1
,
â‹¯
,
ğ‘
âˆ’
1
ğ‘
}
:

ğ±
â€‹
(
ğ‘¡
+
Î”
â€‹
ğ‘¡
)
=
1
âˆ’
ğ›½
â€‹
(
ğ‘¡
+
Î”
â€‹
ğ‘¡
)
â€‹
Î”
â€‹
ğ‘¡
â€‹
ğ±
â€‹
(
ğ‘¡
)
+
ğ›½
â€‹
(
ğ‘¡
+
Î”
â€‹
ğ‘¡
)
â€‹
Î”
â€‹
ğ‘¡
â€‹
ğ³
â€‹
(
ğ‘¡
)
â‰ˆ
ğ±
â€‹
(
ğ‘¡
)
âˆ’
1
2
â€‹
ğ›½
â€‹
(
ğ‘¡
+
Î”
â€‹
ğ‘¡
)
â€‹
Î”
â€‹
ğ‘¡
â€‹
ğ±
â€‹
(
ğ‘¡
)
+
ğ›½
â€‹
(
ğ‘¡
+
Î”
â€‹
ğ‘¡
)
â€‹
Î”
â€‹
ğ‘¡
â€‹
ğ³
â€‹
(
ğ‘¡
)
â‰ˆ
ğ±
â€‹
(
ğ‘¡
)
âˆ’
1
2
â€‹
ğ›½
â€‹
(
ğ‘¡
)
â€‹
Î”
â€‹
ğ‘¡
â€‹
ğ±
â€‹
(
ğ‘¡
)
+
ğ›½
â€‹
(
ğ‘¡
)
â€‹
Î”
â€‹
ğ‘¡
â€‹
ğ³
â€‹
(
ğ‘¡
)
,
(24)
where the approximate equality holds when 
Î”
â€‹
ğ‘¡
â‰ª
1
. Therefore, in the limit of 
Î”
â€‹
ğ‘¡
â†’
0
, Eq. 24 converges to the following VP SDE:

d
â€‹
ğ±
=
âˆ’
1
2
â€‹
ğ›½
â€‹
(
ğ‘¡
)
â€‹
ğ±
â€‹
d
â€‹
ğ‘¡
+
ğ›½
â€‹
(
ğ‘¡
)
â€‹
d
â€‹
ğ°
.
(25)
So far, we have demonstrated that the noise perturbations used in SMLD and DDPM correspond to discretizations of VE and VP SDEs respectively. The VE SDE always yields a process with exploding variance when 
ğ‘¡
â†’
âˆ
. In contrast, the VP SDE yields a process with bounded variance. In addition, the process has a constant unit variance for all 
ğ‘¡
âˆˆ
[
0
,
âˆ
)
 when 
ğ‘
â€‹
(
ğ±
â€‹
(
0
)
)
 has a unit variance. Since the VP SDE has affine drift and diffusion coefficients, we can use Eq. (5.51) in SÃ¤rkkÃ¤ & Solin (2019) to obtain an ODE that governs the evolution of variance

d
â€‹
ğšº
VP
â€‹
(
ğ‘¡
)
d
â€‹
ğ‘¡
=
ğ›½
â€‹
(
ğ‘¡
)
â€‹
(
ğˆ
âˆ’
ğšº
VP
â€‹
(
ğ‘¡
)
)
,
where 
ğšº
VP
â€‹
(
ğ‘¡
)
â‰”
Cov
â¡
[
ğ±
â€‹
(
ğ‘¡
)
]
 for 
{
ğ±
â€‹
(
ğ‘¡
)
}
ğ‘¡
=
0
1
 obeying a VP SDE. Solving this ODE, we obtain

ğšº
VP
â€‹
(
ğ‘¡
)
=
ğˆ
+
ğ‘’
âˆ«
0
ğ‘¡
âˆ’
ğ›½
â€‹
(
ğ‘ 
)
â€‹
d
â€‹
ğ‘ 
â€‹
(
ğšº
VP
â€‹
(
0
)
âˆ’
ğˆ
)
,
(26)
from which it is clear that the variance 
ğšº
VP
â€‹
(
ğ‘¡
)
 is always bounded given 
ğšº
VP
â€‹
(
0
)
. Moreover, 
ğšº
VP
â€‹
(
ğ‘¡
)
â‰¡
ğˆ
 if 
ğšº
VP
â€‹
(
0
)
=
ğˆ
. Due to this difference, we name Eq. 9 as the Variance Exploding (VE) SDE, and Eq. 11 the Variance Preserving (VP) SDE.

Inspired by the VP SDE, we propose a new SDE called the sub-VP SDE, namely

d
â€‹
ğ±
=
âˆ’
1
2
â€‹
ğ›½
â€‹
(
ğ‘¡
)
â€‹
ğ±
â€‹
d
â€‹
ğ‘¡
+
ğ›½
â€‹
(
ğ‘¡
)
â€‹
(
1
âˆ’
ğ‘’
âˆ’
2
â€‹
âˆ«
0
ğ‘¡
ğ›½
â€‹
(
ğ‘ 
)
â€‹
d
ğ‘ 
)
â€‹
d
â€‹
ğ°
.
(27)
Following standard derivations, it is straightforward to show that 
ğ”¼
â€‹
[
ğ±
â€‹
(
ğ‘¡
)
]
 is the same for both VP and sub-VP SDEs; the variance function of sub-VP SDEs is different, given by

ğšº
sub-VP
â€‹
(
ğ‘¡
)
=
ğˆ
+
ğ‘’
âˆ’
2
â€‹
âˆ«
0
ğ‘¡
ğ›½
â€‹
(
ğ‘ 
)
â€‹
d
ğ‘ 
â€‹
ğˆ
+
ğ‘’
âˆ’
âˆ«
0
ğ‘¡
ğ›½
â€‹
(
ğ‘ 
)
â€‹
d
ğ‘ 
â€‹
(
ğšº
sub-VP
â€‹
(
0
)
âˆ’
2
â€‹
ğˆ
)
,
(28)
where 
ğšº
sub-VP
â€‹
(
ğ‘¡
)
â‰”
Cov
â¡
[
ğ±
â€‹
(
ğ‘¡
)
]
 for a process 
{
ğ±
â€‹
(
ğ‘¡
)
}
ğ‘¡
=
0
1
 obtained by solving Eq. 27. In addition, we observe that (i) 
ğšº
sub-VP
â€‹
(
ğ‘¡
)
â‰¼
ğšº
VP
â€‹
(
ğ‘¡
)
 for all 
ğ‘¡
â‰¥
0
 with 
ğšº
sub-VP
â€‹
(
0
)
=
ğšº
VP
â€‹
(
0
)
 and shared 
ğ›½
â€‹
(
ğ‘ 
)
; and (ii) 
lim
ğ‘¡
â†’
âˆ
ğšº
sub-VP
â€‹
(
ğ‘¡
)
=
lim
ğ‘¡
â†’
âˆ
ğšº
VP
â€‹
(
ğ‘¡
)
=
ğˆ
 if 
lim
ğ‘¡
â†’
âˆ
âˆ«
0
ğ‘¡
ğ›½
â€‹
(
ğ‘ 
)
â€‹
d
ğ‘ 
=
âˆ
. The former is why we name Eq. 27 the sub-VP SDEâ€”its variance is always upper bounded by the corresponding VP SDE. The latter justifies the use of sub-VP SDEs for score-based generative modeling, since they can perturb any data distribution to standard Gaussian under suitable conditions, just like VP SDEs.

VE, VP and sub-VP SDEs all have affine drift coefficients. Therefore, their perturbation kernels 
ğ‘
0
â€‹
ğ‘¡
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
âˆ£
ğ±
â€‹
(
0
)
)
 are all Gaussian and can be computed with Eqs. (5.50) and (5.51) in SÃ¤rkkÃ¤ & Solin (2019):

ğ‘
0
â€‹
ğ‘¡
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
âˆ£
ğ±
â€‹
(
0
)
)
=
{
ğ’©
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
;
ğ±
â€‹
(
0
)
,
[
ğœ
2
â€‹
(
ğ‘¡
)
âˆ’
ğœ
2
â€‹
(
0
)
]
â€‹
ğˆ
)
,
(VE SDE)
ğ’©
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
;
ğ±
â€‹
(
0
)
â€‹
ğ‘’
âˆ’
1
2
â€‹
âˆ«
0
ğ‘¡
ğ›½
â€‹
(
ğ‘ 
)
â€‹
d
ğ‘ 
,
ğˆ
âˆ’
ğˆ
â€‹
ğ‘’
âˆ’
âˆ«
0
ğ‘¡
ğ›½
â€‹
(
ğ‘ 
)
â€‹
d
ğ‘ 
)
(VP SDE)
ğ’©
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
;
ğ±
â€‹
(
0
)
â€‹
ğ‘’
âˆ’
1
2
â€‹
âˆ«
0
ğ‘¡
ğ›½
â€‹
(
ğ‘ 
)
â€‹
d
ğ‘ 
,
[
1
âˆ’
ğ‘’
âˆ’
âˆ«
0
ğ‘¡
ğ›½
â€‹
(
ğ‘ 
)
â€‹
d
ğ‘ 
]
2
â€‹
ğˆ
)
(sub-VP SDE)
.
(29)
As a result, all SDEs introduced here can be efficiently trained with the objective in Eq. 7.

Appendix CSDEs in the wild
Below we discuss concrete instantiations of VE and VP SDEs whose discretizations yield SMLD and DDPM models, and the specific sub-VP SDE used in our experiments. In SMLD, the noise scales 
{
ğœ
ğ‘–
}
ğ‘–
=
1
ğ‘
 is typically a geometric sequence where 
ğœ
min
 is fixed to 
0.01
 and 
ğœ
max
 is chosen according to Technique 1 in Song & Ermon (2020). Usually, SMLD models normalize image inputs to the range 
[
0
,
1
]
. Since 
{
ğœ
ğ‘–
}
ğ‘–
=
1
ğ‘
 is a geometric sequence, we have 
ğœ
â€‹
(
ğ‘–
ğ‘
)
=
ğœ
ğ‘–
=
ğœ
min
â€‹
(
ğœ
max
ğœ
min
)
ğ‘–
âˆ’
1
ğ‘
âˆ’
1
 for 
ğ‘–
=
1
,
2
,
â‹¯
,
ğ‘
. In the limit of 
ğ‘
â†’
âˆ
, we have 
ğœ
â€‹
(
ğ‘¡
)
=
ğœ
min
â€‹
(
ğœ
max
ğœ
min
)
ğ‘¡
 for 
ğ‘¡
âˆˆ
(
0
,
1
]
. The corresponding VE SDE is

d
â€‹
ğ±
=
ğœ
min
â€‹
(
ğœ
max
ğœ
min
)
ğ‘¡
â€‹
2
â€‹
log
â¡
ğœ
max
ğœ
min
â€‹
d
â€‹
ğ°
,
ğ‘¡
âˆˆ
(
0
,
1
]
,
(30)
and the perturbation kernel can be derived via Eq. 29:

ğ‘
0
â€‹
ğ‘¡
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
âˆ£
ğ±
â€‹
(
0
)
)
=
ğ’©
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
;
ğ±
â€‹
(
0
)
,
ğœ
min
2
â€‹
(
ğœ
max
ğœ
min
)
2
â€‹
ğ‘¡
â€‹
ğˆ
)
,
ğ‘¡
âˆˆ
(
0
,
1
]
.
(31)
There is one subtlety when 
ğ‘¡
=
0
: by definition, 
ğœ
â€‹
(
0
)
=
ğœ
0
=
0
 (following the convention in Eq. 20), but 
ğœ
â€‹
(
0
+
)
â‰”
lim
ğ‘¡
â†’
0
+
ğœ
â€‹
(
ğ‘¡
)
=
ğœ
min
â‰ 
0
. In other words, 
ğœ
â€‹
(
ğ‘¡
)
 for SMLD is not differentiable since 
ğœ
â€‹
(
0
)
â‰ 
ğœ
â€‹
(
0
+
)
, causing the VE SDE in Eq. 21 undefined for 
ğ‘¡
=
0
. In practice, we bypass this issue by always solving the SDE and its associated probability flow ODE in the range 
ğ‘¡
âˆˆ
[
ğœ–
,
1
]
 for some small constant 
ğœ–
>
0
, and we use 
ğœ–
=
10
âˆ’
5
 in our VE SDE experiments.

For DDPM models, 
{
ğ›½
ğ‘–
}
ğ‘–
=
1
ğ‘
 is typically an arithmetic sequence where 
ğ›½
ğ‘–
=
ğ›½
Â¯
min
ğ‘
+
ğ‘–
âˆ’
1
ğ‘
â€‹
(
ğ‘
âˆ’
1
)
â€‹
(
ğ›½
Â¯
max
âˆ’
ğ›½
Â¯
min
)
 for 
ğ‘–
=
1
,
2
,
â‹¯
,
ğ‘
. Therefore, 
ğ›½
â€‹
(
ğ‘¡
)
=
ğ›½
Â¯
min
+
ğ‘¡
â€‹
(
ğ›½
Â¯
max
âˆ’
ğ›½
Â¯
min
)
 for 
ğ‘¡
âˆˆ
[
0
,
1
]
 in the limit of 
ğ‘
â†’
âˆ
. This corresponds to the following instantiation of the VP SDE:

d
â€‹
ğ±
=
âˆ’
1
2
â€‹
(
ğ›½
Â¯
min
+
ğ‘¡
â€‹
(
ğ›½
Â¯
max
âˆ’
ğ›½
Â¯
min
)
)
â€‹
ğ±
â€‹
d
â€‹
ğ‘¡
+
ğ›½
Â¯
min
+
ğ‘¡
â€‹
(
ğ›½
Â¯
max
âˆ’
ğ›½
Â¯
min
)
â€‹
d
â€‹
ğ°
,
ğ‘¡
âˆˆ
[
0
,
1
]
,
(32)
where 
ğ±
â€‹
(
0
)
âˆ¼
ğ‘
data
â€‹
(
ğ±
)
. In our experiments, we let 
ğ›½
Â¯
min
=
0.1
 and 
ğ›½
Â¯
max
=
20
 to match the settings in Ho et al. (2020). The perturbation kernel is given by

ğ‘
0
â€‹
ğ‘¡
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
âˆ£
ğ±
â€‹
(
0
)
)
=
ğ’©
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
;
ğ‘’
âˆ’
1
4
â€‹
ğ‘¡
2
â€‹
(
ğ›½
Â¯
max
âˆ’
ğ›½
Â¯
min
)
âˆ’
1
2
â€‹
ğ‘¡
â€‹
ğ›½
Â¯
min
â€‹
ğ±
â€‹
(
0
)
,
ğˆ
âˆ’
ğˆ
â€‹
ğ‘’
âˆ’
1
2
â€‹
ğ‘¡
2
â€‹
(
ğ›½
Â¯
max
âˆ’
ğ›½
Â¯
min
)
âˆ’
ğ‘¡
â€‹
ğ›½
Â¯
min
)
,
ğ‘¡
âˆˆ
[
0
,
1
]
.
(33)
For DDPM, there is no discontinuity issue with the corresponding VP SDE; yet, there are numerical instability issues for training and sampling at 
ğ‘¡
=
0
, due to the vanishing variance of 
ğ±
â€‹
(
ğ‘¡
)
 as 
ğ‘¡
â†’
0
. Therefore, same as the VE SDE, we restrict computation to 
ğ‘¡
âˆˆ
[
ğœ–
,
1
]
 for a small 
ğœ–
>
0
. For sampling, we choose 
ğœ–
=
10
âˆ’
3
 so that the variance of 
ğ±
â€‹
(
ğœ–
)
 in VP SDE matches the variance of 
ğ±
1
 in DDPM; for training and likelihood computation, we adopt 
ğœ–
=
10
âˆ’
5
 which empirically gives better results.

Refer to caption
(a)SMLD
Refer to caption
(b)DDPM (mean)
Refer to caption
(c)DDPM (variance)
Figure 5:Discrete-time perturbation kernels and our continuous generalizations match each other almost exactly. (a) compares the variance of perturbation kernels for SMLD and VE SDE; (b) compares the scaling factors of means of perturbation kernels for DDPM and VP SDE; and (c) compares the variance of perturbation kernels for DDPM and VP SDE.
As a sanity check for our SDE generalizations to SMLD and DDPM, we compare the perturbation kernels of SDEs and original discrete Markov chains in Fig. 5. The SMLD and DDPM models both use 
ğ‘
=
1000
 noise scales. For SMLD, we only need to compare the variances of perturbation kernels since means are the same by definition. For DDPM, we compare the scaling factors of means and the variances. As demonstrated in Fig. 5, the discrete perturbation kernels of original SMLD and DDPM models align well with perturbation kernels derived from VE and VP SDEs.

For sub-VP SDEs, we use exactly the same 
ğ›½
â€‹
(
ğ‘¡
)
 as VP SDEs. This leads to the following perturbation kernel

ğ‘
0
â€‹
ğ‘¡
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
âˆ£
ğ±
â€‹
(
0
)
)
=
ğ’©
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
;
ğ‘’
âˆ’
1
4
â€‹
ğ‘¡
2
â€‹
(
ğ›½
Â¯
max
âˆ’
ğ›½
Â¯
min
)
âˆ’
1
2
â€‹
ğ‘¡
â€‹
ğ›½
Â¯
min
â€‹
ğ±
â€‹
(
0
)
,
[
1
âˆ’
ğ‘’
âˆ’
1
2
â€‹
ğ‘¡
2
â€‹
(
ğ›½
Â¯
max
âˆ’
ğ›½
Â¯
min
)
âˆ’
ğ‘¡
â€‹
ğ›½
Â¯
min
]
2
â€‹
ğˆ
)
,
ğ‘¡
âˆˆ
[
0
,
1
]
.
(34)
We also restrict numerical computation to the same interval of 
[
ğœ–
,
1
]
 as VP SDEs.

Empirically, we observe that smaller 
ğœ–
 generally yields better likelihood values for all SDEs. For sampling, it is important to use an appropriate 
ğœ–
 for better Inception scores and FIDs, although samples across different 
ğœ–
 look visually the same to human eyes.

Appendix DProbability flow ODE
D.1Derivation
The idea of probability flow ODE is inspired by Maoutsa et al. (2020), and one can find the derivation of a simplified case therein. Below we provide a derivation for the fully general ODE in Eq. 17. We consider the SDE in Eq. 15, which possesses the following form:

d
â€‹
ğ±
=
ğŸ
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
d
â€‹
ğ‘¡
+
ğ†
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
d
â€‹
ğ°
,
where 
ğŸ
â€‹
(
â‹…
,
ğ‘¡
)
:
â„
ğ‘‘
â†’
â„
ğ‘‘
 and 
ğ†
â€‹
(
â‹…
,
ğ‘¡
)
:
â„
ğ‘‘
â†’
â„
ğ‘‘
Ã—
ğ‘‘
. The marginal probability density 
ğ‘
ğ‘¡
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
)
 evolves according to Kolmogorovâ€™s forward equation (Fokker-Planck equation) (Ã˜ksendal, 2003)

âˆ‚
ğ‘
ğ‘¡
â€‹
(
ğ±
)
âˆ‚
ğ‘¡
=
âˆ’
âˆ‘
ğ‘–
=
1
ğ‘‘
âˆ‚
âˆ‚
ğ‘¥
ğ‘–
â€‹
[
ğ‘“
ğ‘–
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
ğ‘
ğ‘¡
â€‹
(
ğ±
)
]
+
1
2
â€‹
âˆ‘
ğ‘–
=
1
ğ‘‘
âˆ‘
ğ‘—
=
1
ğ‘‘
âˆ‚
2
âˆ‚
ğ‘¥
ğ‘–
â€‹
âˆ‚
ğ‘¥
ğ‘—
â€‹
[
âˆ‘
ğ‘˜
=
1
ğ‘‘
ğº
ğ‘–
â€‹
ğ‘˜
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
ğº
ğ‘—
â€‹
ğ‘˜
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
ğ‘
ğ‘¡
â€‹
(
ğ±
)
]
.
(35)
We can easily rewrite Eq. 35 to obtain

âˆ‚
ğ‘
ğ‘¡
â€‹
(
ğ±
)
âˆ‚
ğ‘¡
=
âˆ’
âˆ‘
ğ‘–
=
1
ğ‘‘
âˆ‚
âˆ‚
ğ‘¥
ğ‘–
â€‹
[
ğ‘“
ğ‘–
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
ğ‘
ğ‘¡
â€‹
(
ğ±
)
]
+
1
2
â€‹
âˆ‘
ğ‘–
=
1
ğ‘‘
âˆ‘
ğ‘—
=
1
ğ‘‘
âˆ‚
2
âˆ‚
ğ‘¥
ğ‘–
â€‹
âˆ‚
ğ‘¥
ğ‘—
â€‹
[
âˆ‘
ğ‘˜
=
1
ğ‘‘
ğº
ğ‘–
â€‹
ğ‘˜
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
ğº
ğ‘—
â€‹
ğ‘˜
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
ğ‘
ğ‘¡
â€‹
(
ğ±
)
]
=
âˆ’
âˆ‘
ğ‘–
=
1
ğ‘‘
âˆ‚
âˆ‚
ğ‘¥
ğ‘–
â€‹
[
ğ‘“
ğ‘–
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
ğ‘
ğ‘¡
â€‹
(
ğ±
)
]
+
1
2
â€‹
âˆ‘
ğ‘–
=
1
ğ‘‘
âˆ‚
âˆ‚
ğ‘¥
ğ‘–
â€‹
[
âˆ‘
ğ‘—
=
1
ğ‘‘
âˆ‚
âˆ‚
ğ‘¥
ğ‘—
â€‹
[
âˆ‘
ğ‘˜
=
1
ğ‘‘
ğº
ğ‘–
â€‹
ğ‘˜
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
ğº
ğ‘—
â€‹
ğ‘˜
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
ğ‘
ğ‘¡
â€‹
(
ğ±
)
]
]
.
(36)
Note that

âˆ‘
ğ‘—
=
1
ğ‘‘
âˆ‚
âˆ‚
ğ‘¥
ğ‘—
â€‹
[
âˆ‘
ğ‘˜
=
1
ğ‘‘
ğº
ğ‘–
â€‹
ğ‘˜
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
ğº
ğ‘—
â€‹
ğ‘˜
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
ğ‘
ğ‘¡
â€‹
(
ğ±
)
]
=
âˆ‘
ğ‘—
=
1
ğ‘‘
âˆ‚
âˆ‚
ğ‘¥
ğ‘—
â€‹
[
âˆ‘
ğ‘˜
=
1
ğ‘‘
ğº
ğ‘–
â€‹
ğ‘˜
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
ğº
ğ‘—
â€‹
ğ‘˜
â€‹
(
ğ±
,
ğ‘¡
)
]
â€‹
ğ‘
ğ‘¡
â€‹
(
ğ±
)
+
âˆ‘
ğ‘—
=
1
ğ‘‘
âˆ‘
ğ‘˜
=
1
ğ‘‘
ğº
ğ‘–
â€‹
ğ‘˜
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
ğº
ğ‘—
â€‹
ğ‘˜
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
ğ‘
ğ‘¡
â€‹
(
ğ±
)
â€‹
âˆ‚
âˆ‚
ğ‘¥
ğ‘—
â€‹
log
â¡
ğ‘
ğ‘¡
â€‹
(
ğ±
)
=
ğ‘
ğ‘¡
â€‹
(
ğ±
)
â€‹
âˆ‡
â‹…
[
ğ†
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
ğ†
â€‹
(
ğ±
,
ğ‘¡
)
ğ–³
]
+
ğ‘
ğ‘¡
â€‹
(
ğ±
)
â€‹
ğ†
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
ğ†
â€‹
(
ğ±
,
ğ‘¡
)
ğ–³
â€‹
âˆ‡
ğ±
log
â¡
ğ‘
ğ‘¡
â€‹
(
ğ±
)
,
based on which we can continue the rewriting of Eq. 36 to obtain

âˆ‚
ğ‘
ğ‘¡
â€‹
(
ğ±
)
âˆ‚
ğ‘¡
=
âˆ’
âˆ‘
ğ‘–
=
1
ğ‘‘
âˆ‚
âˆ‚
ğ‘¥
ğ‘–
â€‹
[
ğ‘“
ğ‘–
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
ğ‘
ğ‘¡
â€‹
(
ğ±
)
]
+
1
2
â€‹
âˆ‘
ğ‘–
=
1
ğ‘‘
âˆ‚
âˆ‚
ğ‘¥
ğ‘–
â€‹
[
âˆ‘
ğ‘—
=
1
ğ‘‘
âˆ‚
âˆ‚
ğ‘¥
ğ‘—
â€‹
[
âˆ‘
ğ‘˜
=
1
ğ‘‘
ğº
ğ‘–
â€‹
ğ‘˜
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
ğº
ğ‘—
â€‹
ğ‘˜
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
ğ‘
ğ‘¡
â€‹
(
ğ±
)
]
]
=
âˆ’
âˆ‘
ğ‘–
=
1
ğ‘‘
âˆ‚
âˆ‚
ğ‘¥
ğ‘–
â€‹
[
ğ‘“
ğ‘–
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
ğ‘
ğ‘¡
â€‹
(
ğ±
)
]
+
1
2
â€‹
âˆ‘
ğ‘–
=
1
ğ‘‘
âˆ‚
âˆ‚
ğ‘¥
ğ‘–
â€‹
[
ğ‘
ğ‘¡
â€‹
(
ğ±
)
â€‹
âˆ‡
â‹…
[
ğ†
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
ğ†
â€‹
(
ğ±
,
ğ‘¡
)
ğ–³
]
+
ğ‘
ğ‘¡
â€‹
(
ğ±
)
â€‹
ğ†
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
ğ†
â€‹
(
ğ±
,
ğ‘¡
)
ğ–³
â€‹
âˆ‡
ğ±
log
â¡
ğ‘
ğ‘¡
â€‹
(
ğ±
)
]
=
âˆ’
âˆ‘
ğ‘–
=
1
ğ‘‘
âˆ‚
âˆ‚
ğ‘¥
ğ‘–
{
ğ‘“
ğ‘–
(
ğ±
,
ğ‘¡
)
ğ‘
ğ‘¡
(
ğ±
)
âˆ’
1
2
[
âˆ‡
â‹…
[
ğ†
(
ğ±
,
ğ‘¡
)
ğ†
(
ğ±
,
ğ‘¡
)
ğ–³
]
+
ğ†
(
ğ±
,
ğ‘¡
)
ğ†
(
ğ±
,
ğ‘¡
)
ğ–³
âˆ‡
ğ±
log
ğ‘
ğ‘¡
(
ğ±
)
]
ğ‘
ğ‘¡
(
ğ±
)
}
=
âˆ’
âˆ‘
ğ‘–
=
1
ğ‘‘
âˆ‚
âˆ‚
ğ‘¥
ğ‘–
â€‹
[
ğ‘“
~
ğ‘–
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
ğ‘
ğ‘¡
â€‹
(
ğ±
)
]
,
(37)
where we define

ğŸ
~
â€‹
(
ğ±
,
ğ‘¡
)
â‰”
ğŸ
â€‹
(
ğ±
,
ğ‘¡
)
âˆ’
1
2
â€‹
âˆ‡
â‹…
[
ğ†
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
ğ†
â€‹
(
ğ±
,
ğ‘¡
)
ğ–³
]
âˆ’
1
2
â€‹
ğ†
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
ğ†
â€‹
(
ğ±
,
ğ‘¡
)
ğ–³
â€‹
âˆ‡
ğ±
log
â¡
ğ‘
ğ‘¡
â€‹
(
ğ±
)
.
Inspecting Eq. 37, we observe that it equals Kolmogorovâ€™s forward equation of the following SDE with 
ğ†
~
â€‹
(
ğ±
,
ğ‘¡
)
â‰”
ğŸ
 (Kolmogorovâ€™s forward equation in this case is also known as the Liouville equation.)

d
â€‹
ğ±
=
ğŸ
~
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
d
â€‹
ğ‘¡
+
ğ†
~
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
d
â€‹
ğ°
,
which is essentially an ODE:

d
â€‹
ğ±
=
ğŸ
~
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
d
â€‹
ğ‘¡
,
same as the probability flow ODE given by Eq. 17. Therefore, we have shown that the probability flow ODE Eq. 17 induces the same marginal probability density 
ğ‘
ğ‘¡
â€‹
(
ğ±
)
 as the SDE in Eq. 15.

D.2Likelihood computation
The probability flow ODE in Eq. 17 has the following form when we replace the score 
âˆ‡
ğ±
log
â¡
ğ‘
ğ‘¡
â€‹
(
ğ±
)
 with the time-dependent score-based model 
ğ¬
ğœ½
â€‹
(
ğ±
,
ğ‘¡
)
:

d
â€‹
ğ±
=
{
ğŸ
â€‹
(
ğ±
,
ğ‘¡
)
âˆ’
1
2
â€‹
âˆ‡
â‹…
[
ğ†
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
ğ†
â€‹
(
ğ±
,
ğ‘¡
)
ğ–³
]
âˆ’
1
2
â€‹
ğ†
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
ğ†
â€‹
(
ğ±
,
ğ‘¡
)
ğ–³
â€‹
ğ¬
ğœ½
â€‹
(
ğ±
,
ğ‘¡
)
}
âŸ
=
â£
:
ğŸ
~
ğœ½
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
d
â€‹
ğ‘¡
.
(38)
With the instantaneous change of variables formula (Chen et al., 2018), we can compute the log-likelihood of 
ğ‘
0
â€‹
(
ğ±
)
 using

log
â¡
ğ‘
0
â€‹
(
ğ±
â€‹
(
0
)
)
=
log
â¡
ğ‘
ğ‘‡
â€‹
(
ğ±
â€‹
(
ğ‘‡
)
)
+
âˆ«
0
ğ‘‡
âˆ‡
â‹…
ğŸ
~
ğœ½
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
,
ğ‘¡
)
â€‹
d
ğ‘¡
,
(39)
where the random variable 
ğ±
â€‹
(
ğ‘¡
)
 as a function of 
ğ‘¡
 can be obtained by solving the probability flow ODE in Eq. 38. In many cases computing 
âˆ‡
â‹…
ğŸ
~
ğœ½
â€‹
(
ğ±
,
ğ‘¡
)
 is expensive, so we follow Grathwohl et al. (2018) to estimate it with the Skilling-Hutchinson trace estimator (Skilling, 1989; Hutchinson, 1990). In particular, we have

âˆ‡
â‹…
ğŸ
~
ğœ½
â€‹
(
ğ±
,
ğ‘¡
)
=
ğ”¼
ğ‘
â€‹
(
ğœ–
)
â€‹
[
ğœ–
ğ–³
â€‹
âˆ‡
ğŸ
~
ğœ½
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
ğœ–
]
,
(40)
where 
âˆ‡
ğŸ
~
ğœ½
 denotes the Jacobian of 
ğŸ
~
ğœ½
â€‹
(
â‹…
,
ğ‘¡
)
, and the random variable 
ğœ–
 satisfies 
ğ”¼
ğ‘
â€‹
(
ğœ–
)
â€‹
[
ğœ–
]
=
ğŸ
 and 
Cov
ğ‘
â€‹
(
ğœ–
)
â¡
[
ğœ–
]
=
ğˆ
. The vector-Jacobian product 
ğœ–
ğ–³
â€‹
âˆ‡
ğŸ
~
ğœ½
â€‹
(
ğ±
,
ğ‘¡
)
 can be efficiently computed using reverse-mode automatic differentiation, at approximately the same cost as evaluating 
ğŸ
~
ğœ½
â€‹
(
ğ±
,
ğ‘¡
)
. As a result, we can sample 
ğœ–
âˆ¼
ğ‘
â€‹
(
ğœ–
)
 and then compute an efficient unbiased estimate to 
âˆ‡
â‹…
ğŸ
~
ğœ½
â€‹
(
ğ±
,
ğ‘¡
)
 using 
ğœ–
ğ–³
â€‹
âˆ‡
ğŸ
~
ğœ½
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
ğœ–
. Since this estimator is unbiased, we can attain an arbitrarily small error by averaging over a sufficient number of runs. Therefore, by applying the Skilling-Hutchinson estimator Eq. 40 to Eq. 39, we can compute the log-likelihood to any accuracy.

In our experiments, we use the RK45 ODE solver (Dormand & Prince, 1980) provided by scipy.integrate.solve_ivp in all cases. The bits/dim values in Table 2 are computed with atol=1e-5 and rtol=1e-5, same as Grathwohl et al. (2018). To give the likelihood results of our models in Table 2, we average the bits/dim obtained on the test dataset over five different runs with 
ğœ–
=
10
âˆ’
5
 (see definition of 
ğœ–
 in Appendix C).

D.3Probability flow sampling
Suppose we have a forward SDE

d
â€‹
ğ±
=
ğŸ
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
d
â€‹
ğ‘¡
+
ğ†
â€‹
(
ğ‘¡
)
â€‹
d
â€‹
ğ°
,
and one of its discretization

ğ±
ğ‘–
+
1
=
ğ±
ğ‘–
+
ğŸ
ğ‘–
â€‹
(
ğ±
ğ‘–
)
+
ğ†
ğ‘–
â€‹
ğ³
ğ‘–
,
ğ‘–
=
0
,
1
,
â‹¯
,
ğ‘
âˆ’
1
,
(41)
where 
ğ³
ğ‘–
âˆ¼
ğ’©
â€‹
(
ğŸ
,
ğˆ
)
. We assume the discretization schedule of time is fixed beforehand, and thus we absorb the dependency on 
Î”
â€‹
ğ‘¡
 into the notations of 
ğŸ
ğ‘–
 and 
ğ†
ğ‘–
. Using Eq. 17, we can obtain the following probability flow ODE:

d
â€‹
ğ±
=
{
ğŸ
â€‹
(
ğ±
,
ğ‘¡
)
âˆ’
1
2
â€‹
ğ†
â€‹
(
ğ‘¡
)
â€‹
ğ†
â€‹
(
ğ‘¡
)
ğ–³
â€‹
âˆ‡
ğ±
log
â¡
ğ‘
ğ‘¡
â€‹
(
ğ±
)
}
â€‹
d
â€‹
ğ‘¡
.
(42)
We may employ any numerical method to integrate the probability flow ODE backwards in time for sample generation. In particular, we propose a discretization in a similar functional form to Eq. 41:

ğ±
ğ‘–
=
ğ±
ğ‘–
+
1
âˆ’
ğŸ
ğ‘–
+
1
â€‹
(
ğ±
ğ‘–
+
1
)
+
1
2
â€‹
ğ†
ğ‘–
+
1
â€‹
ğ†
ğ‘–
+
1
ğ–³
â€‹
ğ¬
ğœ½
âˆ—
â€‹
(
ğ±
ğ‘–
+
1
,
ğ‘–
+
1
)
,
ğ‘–
=
0
,
1
,
â‹¯
,
ğ‘
âˆ’
1
,
where the score-based model 
ğ¬
ğœ½
âˆ—
â€‹
(
ğ±
ğ‘–
,
ğ‘–
)
 is conditioned on the iteration number 
ğ‘–
. This is a deterministic iteration rule. Unlike reverse diffusion samplers or ancestral sampling, there is no additional randomness once the initial sample 
ğ±
ğ‘
 is obtained from the prior distribution. When applied to SMLD models, we can get the following iteration rule for probability flow sampling:

ğ±
ğ‘–
=
ğ±
ğ‘–
+
1
+
1
2
â€‹
(
ğœ
ğ‘–
+
1
2
âˆ’
ğœ
ğ‘–
2
)
â€‹
ğ¬
ğœ½
âˆ—
â€‹
(
ğ±
ğ‘–
+
1
,
ğœ
ğ‘–
+
1
)
,
ğ‘–
=
0
,
1
,
â‹¯
,
ğ‘
âˆ’
1
.
(43)
Similarly, for DDPM models, we have

ğ±
ğ‘–
=
(
2
âˆ’
1
âˆ’
ğ›½
ğ‘–
+
1
)
â€‹
ğ±
ğ‘–
+
1
+
1
2
â€‹
ğ›½
ğ‘–
+
1
â€‹
ğ¬
ğœ½
âˆ—
â€‹
(
ğ±
ğ‘–
+
1
,
ğ‘–
+
1
)
,
ğ‘–
=
0
,
1
,
â‹¯
,
ğ‘
âˆ’
1
.
(44)
D.4Sampling with black-box ODE solvers
For producing figures in Fig. 3, we use a DDPM model trained on 
256
Ã—
256
 CelebA-HQ with the same settings in Ho et al. (2020). All FID scores of our models in Table 2 are computed on samples from the RK45 ODE solver implemented in scipy.integrate.solve_ivp with atol=1e-5 and rtol=1e-5. We use 
ğœ–
=
10
âˆ’
5
 for VE SDEs and 
ğœ–
=
10
âˆ’
3
 for VP SDEs (see also Appendix C).

Aside from the interpolation results in Fig. 3, we demonstrate more examples of latent space manipulation in Fig. 6, including interpolation and temperature scaling. The model tested here is a DDPM model trained with the same settings in Ho et al. (2020).

Refer to caption
Refer to caption
Figure 6:Samples from the probability flow ODE for VP SDE on 
256
Ã—
256
 CelebA-HQ. Top: spherical interpolations between random samples. Bottom: temperature rescaling (reducing norm of embedding).
Although solvers for the probability flow ODE allow fast sampling, their samples typically have higher (worse) FID scores than those from SDE solvers if no corrector is used. We have this empirical observation for both the discretization strategy in Section D.3, and black-box ODE solvers introduced above. Moreover, the performance of probability flow ODE samplers depends on the choice of the SDEâ€”their sample quality for VE SDEs is much worse than VP SDEs especially for high-dimensional data.

D.5Uniquely identifiable encoding
Refer to caption
Figure 7:Comparing the first 100 dimensions of the latent code obtained for a random CIFAR-10 image. â€œModel Aâ€ and â€œModel Bâ€ are separately trained with different architectures.
Refer to caption
Refer to caption
Figure 8:Left: The dimension-wise difference between encodings obtained by Model A and B. As a baseline, we also report the difference between shuffled representations of these two models. Right: The dimension-wise correlation coefficients of encodings obtained by Model A and Model B.
As a sanity check, we train two models (denoted as â€œModel Aâ€ and â€œModel Bâ€) with different architectures using the VE SDE on CIFAR-10. Here Model A is an NCSN++ model with 4 layers per resolution trained using the continuous objective in Eq. 7, and Model B is all the same except that it uses 8 layers per resolution. Model definitions are in Appendix H.

We report the latent codes obtained by Model A and Model B for a random CIFAR-10 image in Fig. 7. In Fig. 8, we show the dimension-wise differences and correlation coefficients between latent encodings on a total of 16 CIFAR-10 images. Our results demonstrate that for the same inputs, Model A and Model B provide encodings that are close in every dimension, despite having different model architectures and training runs.

Appendix EReverse diffusion sampling
Given a forward SDE

d
â€‹
ğ±
=
ğŸ
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
d
â€‹
ğ‘¡
+
ğ†
â€‹
(
ğ‘¡
)
â€‹
d
â€‹
ğ°
,
and suppose the following iteration rule is a discretization of it:

ğ±
ğ‘–
+
1
=
ğ±
ğ‘–
+
ğŸ
ğ‘–
â€‹
(
ğ±
ğ‘–
)
+
ğ†
ğ‘–
â€‹
ğ³
ğ‘–
,
ğ‘–
=
0
,
1
,
â‹¯
,
ğ‘
âˆ’
1
(45)
where 
ğ³
ğ‘–
âˆ¼
ğ’©
â€‹
(
ğŸ
,
ğˆ
)
. Here we assume the discretization schedule of time is fixed beforehand, and thus we can absorb it into the notations of 
ğŸ
ğ‘–
 and 
ğ†
ğ‘–
.

Based on Eq. 45, we propose to discretize the reverse-time SDE

d
â€‹
ğ±
=
[
ğŸ
â€‹
(
ğ±
,
ğ‘¡
)
âˆ’
ğ†
â€‹
(
ğ‘¡
)
â€‹
ğ†
â€‹
(
ğ‘¡
)
ğ–³
â€‹
âˆ‡
ğ±
log
â¡
ğ‘
ğ‘¡
â€‹
(
ğ±
)
]
â€‹
d
â€‹
ğ‘¡
+
ğ†
â€‹
(
ğ‘¡
)
â€‹
d
â€‹
ğ°
Â¯
,
with a similar functional form, which gives the following iteration rule for 
ğ‘–
âˆˆ
{
0
,
1
,
â‹¯
,
ğ‘
âˆ’
1
}
:

ğ±
ğ‘–
=
ğ±
ğ‘–
+
1
âˆ’
ğŸ
ğ‘–
+
1
â€‹
(
ğ±
ğ‘–
+
1
)
+
ğ†
ğ‘–
+
1
â€‹
ğ†
ğ‘–
+
1
ğ–³
â€‹
ğ¬
ğœ½
âˆ—
â€‹
(
ğ±
ğ‘–
+
1
,
ğ‘–
+
1
)
+
ğ†
ğ‘–
+
1
â€‹
ğ³
ğ‘–
+
1
,
(46)
where our trained score-based model 
ğ¬
ğœ½
âˆ—
â€‹
(
ğ±
ğ‘–
,
ğ‘–
)
 is conditioned on iteration number 
ğ‘–
.

When applying Eq. 46 to Eqs. 20 and 10, we obtain a new set of numerical solvers for the reverse-time VE and VP SDEs, resulting in sampling algorithms as shown in the â€œpredictorâ€ part of Algorithms 2 and 3. We name these sampling methods (that are based on the discretization strategy in Eq. 46) reverse diffusion samplers.

As expected, the ancestral sampling of DDPM (Ho et al., 2020) (Eq. 4) matches its reverse diffusion counterpart when 
ğ›½
ğ‘–
â†’
0
 for all 
ğ‘–
 (which happens when 
Î”
â€‹
ğ‘¡
â†’
0
 since 
ğ›½
ğ‘–
=
ğ›½
Â¯
ğ‘–
â€‹
Î”
â€‹
ğ‘¡
, see Appendix B), because

ğ±
ğ‘–
=
1
1
âˆ’
ğ›½
ğ‘–
+
1
â€‹
(
ğ±
ğ‘–
+
1
+
ğ›½
ğ‘–
+
1
â€‹
ğ¬
ğœ½
âˆ—
â€‹
(
ğ±
ğ‘–
+
1
,
ğ‘–
+
1
)
)
+
ğ›½
ğ‘–
+
1
â€‹
ğ³
ğ‘–
+
1
=
(
1
+
1
2
â€‹
ğ›½
ğ‘–
+
1
+
ğ‘œ
â€‹
(
ğ›½
ğ‘–
+
1
)
)
â€‹
(
ğ±
ğ‘–
+
1
+
ğ›½
ğ‘–
+
1
â€‹
ğ¬
ğœ½
âˆ—
â€‹
(
ğ±
ğ‘–
+
1
,
ğ‘–
+
1
)
)
+
ğ›½
ğ‘–
+
1
â€‹
ğ³
ğ‘–
+
1
â‰ˆ
(
1
+
1
2
â€‹
ğ›½
ğ‘–
+
1
)
â€‹
(
ğ±
ğ‘–
+
1
+
ğ›½
ğ‘–
+
1
â€‹
ğ¬
ğœ½
âˆ—
â€‹
(
ğ±
ğ‘–
+
1
,
ğ‘–
+
1
)
)
+
ğ›½
ğ‘–
+
1
â€‹
ğ³
ğ‘–
+
1
=
(
1
+
1
2
â€‹
ğ›½
ğ‘–
+
1
)
â€‹
ğ±
ğ‘–
+
1
+
ğ›½
ğ‘–
+
1
â€‹
ğ¬
ğœ½
âˆ—
â€‹
(
ğ±
ğ‘–
+
1
,
ğ‘–
+
1
)
+
1
2
â€‹
ğ›½
ğ‘–
+
1
2
â€‹
ğ¬
ğœ½
âˆ—
â€‹
(
ğ±
ğ‘–
+
1
,
ğ‘–
+
1
)
+
ğ›½
ğ‘–
+
1
â€‹
ğ³
ğ‘–
+
1
â‰ˆ
(
1
+
1
2
â€‹
ğ›½
ğ‘–
+
1
)
â€‹
ğ±
ğ‘–
+
1
+
ğ›½
ğ‘–
+
1
â€‹
ğ¬
ğœ½
âˆ—
â€‹
(
ğ±
ğ‘–
+
1
,
ğ‘–
+
1
)
+
ğ›½
ğ‘–
+
1
â€‹
ğ³
ğ‘–
+
1
=
[
2
âˆ’
(
1
âˆ’
1
2
â€‹
ğ›½
ğ‘–
+
1
)
]
â€‹
ğ±
ğ‘–
+
1
+
ğ›½
ğ‘–
+
1
â€‹
ğ¬
ğœ½
âˆ—
â€‹
(
ğ±
ğ‘–
+
1
,
ğ‘–
+
1
)
+
ğ›½
ğ‘–
+
1
â€‹
ğ³
ğ‘–
+
1
â‰ˆ
[
2
âˆ’
(
1
âˆ’
1
2
â€‹
ğ›½
ğ‘–
+
1
)
+
ğ‘œ
â€‹
(
ğ›½
ğ‘–
+
1
)
]
â€‹
ğ±
ğ‘–
+
1
+
ğ›½
ğ‘–
+
1
â€‹
ğ¬
ğœ½
âˆ—
â€‹
(
ğ±
ğ‘–
+
1
,
ğ‘–
+
1
)
+
ğ›½
ğ‘–
+
1
â€‹
ğ³
ğ‘–
+
1
=
(
2
âˆ’
1
âˆ’
ğ›½
ğ‘–
+
1
)
â€‹
ğ±
ğ‘–
+
1
+
ğ›½
ğ‘–
+
1
â€‹
ğ¬
ğœ½
âˆ—
â€‹
(
ğ±
ğ‘–
+
1
,
ğ‘–
+
1
)
+
ğ›½
ğ‘–
+
1
â€‹
ğ³
ğ‘–
+
1
.
Therefore, the original ancestral sampler of Eq. 4 is essentially a different discretization to the same reverse-time SDE. This unifies the sampling method in Ho et al. (2020) as a numerical solver to the reverse-time VP SDE in our continuous framework.

Appendix FAncestral sampling for SMLD models
The ancestral sampling method for DDPM models can also be adapted to SMLD models. Consider a sequence of noise scales 
ğœ
1
<
ğœ
2
<
â‹¯
<
ğœ
ğ‘
 as in SMLD. By perturbing a data point 
ğ±
0
 with these noise scales sequentially, we obtain a Markov chain 
ğ±
0
â†’
ğ±
1
â†’
â‹¯
â†’
ğ±
ğ‘
, where

ğ‘
â€‹
(
ğ±
ğ‘–
âˆ£
ğ±
ğ‘–
âˆ’
1
)
=
ğ’©
â€‹
(
ğ±
ğ‘–
;
ğ±
ğ‘–
âˆ’
1
,
(
ğœ
ğ‘–
2
âˆ’
ğœ
ğ‘–
âˆ’
1
2
)
â€‹
ğˆ
)
,
ğ‘–
=
1
,
2
,
â‹¯
,
ğ‘
.
Here we assume 
ğœ
0
=
0
 to simplify notations. Following Ho et al. (2020), we can compute

ğ‘
â€‹
(
ğ±
ğ‘–
âˆ’
1
âˆ£
ğ±
ğ‘–
,
ğ±
0
)
=
ğ’©
â€‹
(
ğ±
ğ‘–
âˆ’
1
;
ğœ
ğ‘–
âˆ’
1
2
ğœ
ğ‘–
2
â€‹
ğ±
ğ‘–
+
(
1
âˆ’
ğœ
ğ‘–
âˆ’
1
2
ğœ
ğ‘–
2
)
â€‹
ğ±
0
,
ğœ
ğ‘–
âˆ’
1
2
â€‹
(
ğœ
ğ‘–
2
âˆ’
ğœ
ğ‘–
âˆ’
1
2
)
ğœ
ğ‘–
2
â€‹
ğˆ
)
.
If we parameterize the reverse transition kernel as 
ğ‘
ğœ½
â€‹
(
ğ±
ğ‘–
âˆ’
1
âˆ£
ğ±
ğ‘–
)
=
ğ’©
â€‹
(
ğ±
ğ‘–
âˆ’
1
;
ğ
ğœ½
â€‹
(
ğ±
ğ‘–
,
ğ‘–
)
,
ğœ
ğ‘–
2
â€‹
ğˆ
)
, then

ğ¿
ğ‘¡
âˆ’
1
=
ğ”¼
ğ‘
[
ğ·
KL
(
ğ‘
(
ğ±
ğ‘–
âˆ’
1
âˆ£
ğ±
ğ‘–
,
ğ±
0
)
)
âˆ¥
ğ‘
ğœ½
(
ğ±
ğ‘–
âˆ’
1
âˆ£
ğ±
ğ‘–
)
]
=
ğ”¼
ğ‘
â€‹
[
1
2
â€‹
ğœ
ğ‘–
2
â€‹
âˆ¥
ğœ
ğ‘–
âˆ’
1
2
ğœ
ğ‘–
2
â€‹
ğ±
ğ‘–
+
(
1
âˆ’
ğœ
ğ‘–
âˆ’
1
2
ğœ
ğ‘–
2
)
â€‹
ğ±
0
âˆ’
ğ
ğœ½
â€‹
(
ğ±
ğ‘–
,
ğ‘–
)
âˆ¥
2
2
]
+
ğ¶
=
ğ”¼
ğ±
0
,
ğ³
â€‹
[
1
2
â€‹
ğœ
ğ‘–
2
â€‹
âˆ¥
ğ±
ğ‘–
â€‹
(
ğ±
0
,
ğ³
)
âˆ’
ğœ
ğ‘–
2
âˆ’
ğœ
ğ‘–
âˆ’
1
2
ğœ
ğ‘–
â€‹
ğ³
âˆ’
ğ
ğœ½
â€‹
(
ğ±
ğ‘–
â€‹
(
ğ±
0
,
ğ³
)
,
ğ‘–
)
âˆ¥
2
2
]
+
ğ¶
,
where 
ğ¿
ğ‘¡
âˆ’
1
 is one representative term in the ELBO objective (see Eq. (8) in Ho et al. (2020)), 
ğ¶
 is a constant that does not depend on 
ğœ½
, 
ğ³
âˆ¼
ğ’©
â€‹
(
ğŸ
,
ğˆ
)
, and 
ğ±
ğ‘–
â€‹
(
ğ±
0
,
ğ³
)
=
ğ±
0
+
ğœ
ğ‘–
â€‹
ğ³
. We can therefore parameterize 
ğ
ğœ½
â€‹
(
ğ±
ğ‘–
,
ğ‘–
)
 via

ğ
ğœ½
â€‹
(
ğ±
ğ‘–
,
ğ‘–
)
=
ğ±
ğ‘–
+
(
ğœ
ğ‘–
2
âˆ’
ğœ
ğ‘–
âˆ’
1
2
)
â€‹
ğ¬
ğœ½
â€‹
(
ğ±
ğ‘–
,
ğ‘–
)
,
where 
ğ¬
ğœ½
â€‹
(
ğ±
ğ‘–
,
ğ‘–
)
 is to estimate 
ğ³
/
ğœ
ğ‘–
. As in Ho et al. (2020), we let 
ğœ
ğ‘–
=
ğœ
ğ‘–
âˆ’
1
2
â€‹
(
ğœ
ğ‘–
2
âˆ’
ğœ
ğ‘–
âˆ’
1
2
)
ğœ
ğ‘–
2
. Through ancestral sampling on 
âˆ
ğ‘–
=
1
ğ‘
ğ‘
ğœ½
â€‹
(
ğ±
ğ‘–
âˆ’
1
âˆ£
ğ±
ğ‘–
)
, we obtain the following iteration rule

ğ±
ğ‘–
âˆ’
1
=
ğ±
ğ‘–
+
(
ğœ
ğ‘–
2
âˆ’
ğœ
ğ‘–
âˆ’
1
2
)
â€‹
ğ¬
ğœ½
âˆ—
â€‹
(
ğ±
ğ‘–
,
ğ‘–
)
+
ğœ
ğ‘–
âˆ’
1
2
â€‹
(
ğœ
ğ‘–
2
âˆ’
ğœ
ğ‘–
âˆ’
1
2
)
ğœ
ğ‘–
2
â€‹
ğ³
ğ‘–
,
ğ‘–
=
1
,
2
,
â‹¯
,
ğ‘
,
(47)
where 
ğ±
ğ‘
âˆ¼
ğ’©
â€‹
(
ğŸ
,
ğœ
ğ‘
2
â€‹
ğˆ
)
, 
ğœ½
âˆ—
 denotes the optimal parameter of 
ğ¬
ğœ½
, and 
ğ³
ğ‘–
âˆ¼
ğ’©
â€‹
(
ğŸ
,
ğˆ
)
. We call Eq. 47 the ancestral sampling method for SMLD models.

Appendix GPredictor-Corrector samplers
Algorithm 1 Predictor-Corrector (PC) sampling
1:
2:
ğ‘
: Number of discretization steps for the reverse-time SDE
3:
ğ‘€
: Number of corrector steps
4:Initialize 
ğ±
ğ‘
âˆ¼
ğ‘
ğ‘‡
â€‹
(
ğ±
)
5:for 
ğ‘–
=
ğ‘
âˆ’
1
 to 
0
 do
6:     
ğ±
ğ‘–
â†
Predictor
â¡
(
ğ±
ğ‘–
+
1
)
7:     for 
ğ‘—
=
1
 to 
ğ‘€
 do
8:         
ğ±
ğ‘–
â†
Corrector
â¡
(
ğ±
ğ‘–
)
      
9:return 
ğ±
0
Predictor-Corrector (PC) sampling
The predictor can be any numerical solver for the reverse-time SDE with a fixed discretization strategy. The corrector can be any score-based MCMC approach. In PC sampling, we alternate between the predictor and corrector, as described in Algorithm 1. For example, when using the reverse diffusion SDE solver (Appendix E) as the predictor, and annealed Langevin dynamics (Song & Ermon, 2019) as the corrector, we have Algorithms 2 and 3 for VE and VP SDEs respectively, where 
{
ğœ–
ğ‘–
}
ğ‘–
=
0
ğ‘
âˆ’
1
 are step sizes for Langevin dynamics as specified below.

Algorithm 2 PC sampling (VE SDE)
1:
ğ±
ğ‘
âˆ¼
ğ’©
â€‹
(
ğŸ
,
ğœ
max
2
â€‹
ğˆ
)
2:for 
ğ‘–
=
ğ‘
âˆ’
1
 to 
0
 do 
[Uncaptioned image]
3:   
ğ±
ğ‘–
â€²
â†
ğ±
ğ‘–
+
1
+
(
ğœ
ğ‘–
+
1
2
âˆ’
ğœ
ğ‘–
2
)
â€‹
ğ¬
ğœ½
âˆ—
â€‹
(
ğ±
ğ‘–
+
1
,
ğœ
ğ‘–
+
1
)
4:   
ğ³
âˆ¼
ğ’©
â€‹
(
ğŸ
,
ğˆ
)
5:   
ğ±
ğ‘–
â†
ğ±
ğ‘–
â€²
+
ğœ
ğ‘–
+
1
2
âˆ’
ğœ
ğ‘–
2
â€‹
ğ³
6:   for 
ğ‘—
=
1
 to 
ğ‘€
 do
7:     
ğ³
âˆ¼
ğ’©
â€‹
(
ğŸ
,
ğˆ
)
8:     
ğ±
ğ‘–
â†
ğ±
ğ‘–
+
ğœ–
ğ‘–
â€‹
ğ¬
ğœ½
âˆ—
â€‹
(
ğ±
ğ‘–
,
ğœ
ğ‘–
)
+
2
â€‹
ğœ–
ğ‘–
â€‹
ğ³
    
9:return 
ğ±
0
Algorithm 3 PC sampling (VP SDE)
1:
ğ±
ğ‘
âˆ¼
ğ’©
â€‹
(
ğŸ
,
ğˆ
)
2:for 
ğ‘–
=
ğ‘
âˆ’
1
 to 
0
 do
3:    
ğ±
ğ‘–
â€²
â†
(
2
âˆ’
1
âˆ’
ğ›½
ğ‘–
+
1
)
â€‹
ğ±
ğ‘–
+
1
+
ğ›½
ğ‘–
+
1
â€‹
ğ¬
ğœ½
âˆ—
â€‹
(
ğ±
ğ‘–
+
1
,
ğ‘–
+
1
)

4:   
ğ³
âˆ¼
ğ’©
â€‹
(
ğŸ
,
ğˆ
)
5:   
ğ±
ğ‘–
â†
ğ±
ğ‘–
â€²
+
ğ›½
ğ‘–
+
1
â€‹
ğ³
6:   for 
ğ‘—
=
1
 to 
ğ‘€
 do
7:     
ğ³
âˆ¼
ğ’©
â€‹
(
ğŸ
,
ğˆ
)
8:     
ğ±
ğ‘–
â†
ğ±
ğ‘–
+
ğœ–
ğ‘–
â€‹
ğ¬
ğœ½
âˆ—
â€‹
(
ğ±
ğ‘–
,
ğ‘–
)
+
2
â€‹
ğœ–
ğ‘–
â€‹
ğ³
    
9:return 
ğ±
0
The corrector algorithms
We take the schedule of annealed Langevin dynamics in Song & Ermon (2019), but re-frame it with slight modifications in order to get better interpretability and empirical performance. We provide the corrector algorithms in Algorithms 4 and 5 respectively, where we call 
ğ‘Ÿ
 the â€œsignal-to-noiseâ€ ratio. We determine the step size 
ğœ–
 using the norm of the Gaussian noise 
âˆ¥
ğ³
âˆ¥
2
, norm of the score-based model 
âˆ¥
ğ¬
ğœ½
âˆ—
âˆ¥
2
 and the signal-to-noise ratio 
ğ‘Ÿ
. When sampling a large batch of samples together, we replace the norm 
âˆ¥
â‹…
âˆ¥
2
 with the average norm across the mini-batch. When the batch size is small, we suggest replacing 
âˆ¥
ğ³
âˆ¥
2
 with 
ğ‘‘
, where 
ğ‘‘
 is the dimensionality of 
ğ³
.

Algorithm 4 Corrector algorithm (VE SDE).
1:
{
ğœ
ğ‘–
}
ğ‘–
=
1
ğ‘
,
ğ‘Ÿ
,
ğ‘
,
ğ‘€
.
2:
ğ±
ğ‘
0
âˆ¼
ğ’©
â€‹
(
ğŸ
,
ğœ
max
2
â€‹
ğˆ
)
3:for 
ğ‘–
â†
ğ‘
 to 
1
 do
4:   for 
ğ‘—
â†
1
 to 
ğ‘€
 do
5:     
ğ³
âˆ¼
ğ’©
â€‹
(
ğŸ
,
ğˆ
)
6:     
ğ 
â†
ğ¬
ğœ½
âˆ—
â€‹
(
ğ±
ğ‘–
ğ‘—
âˆ’
1
,
ğœ
ğ‘–
)
7:     
ğœ–
â†
2
â€‹
(
ğ‘Ÿ
â€‹
âˆ¥
ğ³
âˆ¥
2
/
âˆ¥
ğ 
âˆ¥
2
)
2
8:     
ğ±
ğ‘–
ğ‘—
â†
ğ±
ğ‘–
ğ‘—
âˆ’
1
+
ğœ–
â€‹
ğ 
+
2
â€‹
ğœ–
â€‹
ğ³
    
9:   
ğ±
ğ‘–
âˆ’
1
0
â†
ğ±
ğ‘–
ğ‘€
10:return 
ğ±
0
0
 
Algorithm 5 Corrector algorithm (VP SDE).
1:
{
ğ›½
ğ‘–
}
ğ‘–
=
1
ğ‘
,
{
ğ›¼
ğ‘–
}
ğ‘–
=
1
ğ‘
,
ğ‘Ÿ
,
ğ‘
,
ğ‘€
.
2:
ğ±
ğ‘
0
âˆ¼
ğ’©
â€‹
(
ğŸ
,
ğˆ
)
3:for 
ğ‘–
â†
ğ‘
 to 
1
 do
4:   for 
ğ‘—
â†
1
 to 
ğ‘€
 do
5:     
ğ³
âˆ¼
ğ’©
â€‹
(
ğŸ
,
ğˆ
)
6:     
ğ 
â†
ğ¬
ğœ½
âˆ—
â€‹
(
ğ±
ğ‘–
ğ‘—
âˆ’
1
,
ğ‘–
)
7:     
ğœ–
â†
2
â€‹
ğ›¼
ğ‘–
â€‹
(
ğ‘Ÿ
â€‹
âˆ¥
ğ³
âˆ¥
2
/
âˆ¥
ğ 
âˆ¥
2
)
2
8:     
ğ±
ğ‘–
ğ‘—
â†
ğ±
ğ‘–
ğ‘—
âˆ’
1
+
ğœ–
â€‹
ğ 
+
2
â€‹
ğœ–
â€‹
ğ³
    
9:   
ğ±
ğ‘–
âˆ’
1
0
â†
ğ±
ğ‘–
ğ‘€
10:return 
ğ±
0
0
Denoising
For both SMLD and DDPM models, the generated samples typically contain small noise that is hard to detect by humans. As noted by Jolicoeur-Martineau et al. (2020), FIDs can be significantly worse without removing this noise. This unfortunate sensitivity to noise is also part of the reason why NCSN models trained with SMLD has been performing worse than DDPM models in terms of FID, because the former does not use a denoising step at the end of sampling, while the latter does. In all experiments of this paper we ensure there is a single denoising step at the end of sampling, using Tweedieâ€™s formula (Efron, 2011).

Refer to caption
Refer to caption
Figure 9:PC sampling for LSUN bedroom and church. The vertical axis corresponds to the total computation, and the horizontal axis represents the amount of computation allocated to the corrector. Samples are the best when computation is split between the predictor and corrector.
Training
We use the same architecture in Ho et al. (2020) for our score-based models. For the VE SDE, we train a model with the original SMLD objective in Eq. 1; similarly for the VP SDE, we use the original DDPM objective in Eq. 3. We apply a total number of 1000 noise scales for training both models. For results in Fig. 9, we train an NCSN++ model (definition in Appendix H) on 
256
Ã—
256
 LSUN bedroom and church_outdoor (Yu et al., 2015) datasets with the VE SDE and our continuous objective Eq. 7. The batch size is fixed to 128 on CIFAR-10 and 64 on LSUN.

Ad-hoc interpolation methods for noise scales
Models in this experiment are all trained with 1000 noise scales. To get results for P2000 (predictor-only sampler using 2000 steps) which requires 2000 noise scales, we need to interpolate between 1000 noise scales at test time. The specific architecture of the noise-conditional score-based model in Ho et al. (2020) uses sinusoidal positional embeddings for conditioning on integer time steps. This allows us to interpolate between noise scales at test time in an ad-hoc way (while it is hard to do so for other architectures like the one in Song & Ermon (2019)). Specifically, for SMLD models, we keep 
ğœ
min
 and 
ğœ
max
 fixed and double the number of time steps. For DDPM models, we halve 
ğ›½
min
 and 
ğ›½
max
 before doubling the number of time steps. Suppose 
{
ğ¬
ğœ½
â€‹
(
ğ±
,
ğ‘–
)
}
ğ‘–
=
0
ğ‘
âˆ’
1
 is a score-based model trained on 
ğ‘
 time steps, and let 
{
ğ¬
ğœ½
â€²
â€‹
(
ğ±
,
ğ‘–
)
}
ğ‘–
=
0
2
â€‹
ğ‘
âˆ’
1
 denote the corresponding interpolated score-based model at 
2
â€‹
ğ‘
 time steps. We test two different interpolation strategies for time steps: linear interpolation where 
ğ¬
ğœ½
â€²
â€‹
(
ğ±
,
ğ‘–
)
=
ğ¬
ğœ½
â€‹
(
ğ±
,
ğ‘–
/
2
)
 and rounding interpolation where 
ğ¬
ğœ½
â€²
â€‹
(
ğ±
,
ğ‘–
)
=
ğ¬
ğœ½
â€‹
(
ğ±
,
âŒŠ
ğ‘–
/
2
âŒ‹
)
. We provide results with linear interpolation in Table 1, and give results of rounding interpolation in Table 4. We observe that different interpolation methods result in performance differences but maintain the general trend of predictor-corrector methods performing on par or better than predictor-only or corrector-only samplers.

Hyper-parameters of the samplers
For Predictor-Corrector and corrector-only samplers on CIFAR-10, we search for the best signal-to-noise ratio (
ğ‘Ÿ
) over a grid that increments at 0.01. We report the best 
ğ‘Ÿ
 in Table 5. For LSUN bedroom/church_outdoor, we fix 
ğ‘Ÿ
 to 0.075. Unless otherwise noted, we use one corrector step per noise scale for all PC samplers. We use two corrector steps per noise scale for corrector-only samplers on CIFAR-10. For sample generation, the batch size is 1024 on CIFAR-10 and 8 on LSUN bedroom/church_outdoor.

Table 4:Comparing different samplers on CIFAR-10, where â€œP2000â€ uses the rounding interpolation between noise scales. Shaded regions are obtained with the same computation (number of score function evaluations). Mean and standard deviation are reported over five sampling runs.
  \bigstrut	Variance Exploding SDE (SMLD)	Variance Preserving SDE (DDPM)
 \bigstrut
Predictor
Sampler
FID
â†“
P1000	P2000	C2000	PC1000	P1000	P2000	C2000	PC1000
 \bigstrutancestral sampling	4.98 
Â±
 .06
4.92 
Â±
 .02
3.62 
Â±
 .03
3.24 
Â±
 .02
3.11 
Â±
 .03
3.21 
Â±
 .02
reverse diffusion	4.79 
Â±
 .07
4.72 
Â±
 .07
3.60 
Â±
 .02
3.21 
Â±
 .02
3.10 
Â±
 .03
3.18 
Â±
 .01
probability flow	15.41 
Â±
 .15
12.87 
Â±
 .09
20.43 
Â±
 .07
3.51 
Â±
 .04
3.59 
Â±
 .04
3.25 
Â±
 .04
19.06 
Â±
 .06
3.06 
Â±
 .03
 								

Table 5:Optimal signal-to-noise ratios of different samplers. â€œP1000â€ or â€œP2000â€: predictor-only samplers using 1000 or 2000 steps. â€œC2000â€: corrector-only samplers using 2000 steps. â€œPC1000â€: PC samplers using 1000 predictor and 1000 corrector steps.
  \bigstrut	VE SDE (SMLD)	VP SDE (DDPM)
 \bigstrut
Predictor
Sampler
ğ‘Ÿ
P1000	P2000	C2000	PC1000	P1000	P2000	C2000	PC1000
 \bigstrutancestral sampling	-	-		0.17	-	-		0.01
reverse diffusion	-	-		0.16	-	-		0.01
probability flow	-	-	0.22	0.17	-	-	0.27	0.04
 								

Appendix HArchitecture improvements
We explored several architecture designs to improve score-based models for both VE and VP SDEs. Our endeavor gives rise to new state-of-the-art sample quality on CIFAR-10, new state-of-the-art likelihood on uniformly dequantized CIFAR-10, and enables the first high-fidelity image samples of resolution 
1024
Ã—
1024
 from score-based generative models. Code and checkpoints are open-sourced at https://github.com/yang-song/score_sde.

H.1Settings for architecture exploration
Unless otherwise noted, all models are trained for 1.3M iterations, and we save one checkpoint per 50k iterations. For VE SDEs, we consider two datasets: 
32
Ã—
32
 CIFAR-10 (Krizhevsky et al., 2009) and 
64
Ã—
64
 CelebA (Liu et al., 2015), pre-processed following Song & Ermon (2020). We compare different configurations based on their FID scores averaged over checkpoints after 0.5M iterations. For VP SDEs, we only consider the CIFAR-10 dataset to save computation, and compare models based on the average FID scores over checkpoints obtained between 0.25M and 0.5M iterations, because FIDs turn to increase after 0.5M iterations for VP SDEs.

All FIDs are computed on 50k samples with tensorflow_gan. For sampling, we use the PC sampler discretized at 1000 time steps. We choose reverse diffusion (see Appendix E) as the predictor. We use one corrector step per update of the predictor for VE SDEs with a signal-to-noise ratio of 0.16, but save the corrector step for VP SDEs since correctors there only give slightly better results but require double computation. We follow Ho et al. (2020) for optimization, including the learning rate, gradient clipping, and learning rate warm-up schedules. Unless otherwise noted, models are trained with the original discrete SMLD and DDPM objectives in Eqs. 1 and 3 and use a batch size of 128. The optimal architectures found under these settings are subsequently transferred to continuous objectives and deeper models. We also directly transfer the best architecture for VP SDEs to sub-VP SDEs, given the similarity of these two SDEs.

Refer to caption
Refer to caption
Refer to caption
Refer to caption
Refer to caption
Figure 10:The effects of different architecture components for score-based models trained with VE perturbations.
Our architecture is mostly based on Ho et al. (2020). We additionally introduce the following components to maximize the potential improvement of score-based models.

1. Upsampling and downsampling images with anti-aliasing based on Finite Impulse Response (FIR) (Zhang, 2019). We follow the same implementation and hyper-parameters in StyleGAN-2 (Karras et al., 2020b).
2. Rescaling all skip connections by 
1
/
2
. This has been demonstrated effective in several best-in-class GAN models, including ProgressiveGAN (Karras et al., 2018), StyleGAN (Karras et al., 2019) and StyleGAN-2 (Karras et al., 2020b).
3. Replacing the original residual blocks in DDPM with residual blocks from BigGAN (Brock et al., 2018).
4. Increasing the number of residual blocks per resolution from 
2
 to 
4
.
5. Incorporating progressive growing architectures. We consider two progressive architectures for input: â€œinput skipâ€ and â€œresidualâ€, and two progressive architectures for output: â€œoutput skipâ€ and â€œresidualâ€. These progressive architectures are defined and implemented according to StyleGAN-2.
We also tested equalized learning rates, a trick used in very successful models like ProgressiveGAN (Karras et al., 2018) and StyleGAN (Karras et al., 2019). However, we found it harmful at an early stage of our experiments, and therefore decided not to explore more on it.

The exponential moving average (EMA) rate has a significant impact on performance. For models trained with VE perturbations, we notice that 0.999 works better than 0.9999, whereas for models trained with VP perturbations it is the opposite. We therefore use an EMA rate of 0.999 and 0.9999 for VE and VP models respectively.

Refer to caption
Figure 11:Unconditional CIFAR-10 samples from NCSN++ cont. (deep, VE).
Refer to caption
Figure 12:Samples on 
1024
Ã—
1024
 CelebA-HQ from a modified NCSN++ model trained with the VE SDE.
H.2Results on CIFAR-10
All architecture components introduced above can improve the performance of score-based models trained with VE SDEs, as shown in Fig. 10. The box plots demonstrate the importance of each component when other components can vary freely. On both CIFAR-10 and CelebA, the additional components that we explored always improve the performance on average for VE SDEs. For progressive growing, it is not clear which combination of configurations consistently performs the best, but the results are typically better than when no progressive growing architecture is used. Our best score-based model for VE SDEs 1) uses FIR upsampling/downsampling, 2) rescales skip connections, 3) employs BigGAN-type residual blocks, 4) uses 4 residual blocks per resolution instead of 2, and 5) uses â€œresidualâ€ for input and no progressive growing architecture for output. We name this model â€œNCSN++â€, following the naming convention of previous SMLD models (Song & Ermon, 2019; 2020).

We followed a similar procedure to examine these architecture components for VP SDEs, except that we skipped experiments on CelebA due to limited computing resources. The NCSN++ architecture worked decently well for VP SDEs, ranked 4th place over all 144 possible configurations. The top configuration, however, has a slightly different structure, which uses no FIR upsampling/downsampling and no progressive growing architecture compared to NCSN++. We name this model â€œDDPM++â€, following the naming convention of Ho et al. (2020).

The basic NCSN++ model with 4 residual blocks per resolution achieves an FID of 2.45 on CIFAR-10, whereas the basic DDPM++ model achieves an FID of 2.78. Here in order to match the convention used in Karras et al. (2018); Song & Ermon (2019) and Ho et al. (2020), we report the lowest FID value over the course of training, rather than the average FID value over checkpoints after 0.5M iterations (used for comparing different models of VE SDEs) or between 0.25M and 0.5M iterations (used for comparing VP SDE models) in our architecture exploration.

Switching from discrete training objectives to continuous ones in Eq. 7 further improves the FID values for all SDEs. To condition the NCSN++ model on continuous time variables, we change positional embeddings, the layers in Ho et al. (2020) for conditioning on discrete time steps, to random Fourier feature embeddings (Tancik et al., 2020). The scale parameter of these random Fourier feature embeddings is fixed to 16. We also reduce the number of training iterations to 0.95M to suppress overfitting. These changes improve the FID on CIFAR-10 from 2.45 to 2.38 for NCSN++ trained with the VE SDE, resulting in a model called â€œNCSN++ cont.â€. In addition, we can further improve the FID from 2.38 to 2.20 by doubling the number of residual blocks per resolution for NCSN++ cont., resulting in the model denoted as â€œNCSN++ cont. (deep)â€. All quantitative results are summarized in Table 3, and we provide random samples from our best model in Fig. 11.

Similarly, we can also condition the DDPM++ model on continuous time steps, resulting in a model â€œDDPM++ cont.â€. When trained with the VP SDE, it improves the FID of 2.78 from DDPM++ to 2.55. When trained with the sub-VP SDE, it achieves an FID of 2.61. To get better performance, we used the Euler-Maruyama solver as the predictor for continuously-trained models, instead of the ancestral sampling predictor or the reverse diffusion predictor. This is because the discretization strategy of the original DDPM method does not match the variance of the continuous process well when 
ğ‘¡
â†’
0
, which significantly hurts FID scores. As shown in Table 2, the likelihood values are 3.21 and 3.05 bits/dim for VP and sub-VP SDEs respectively. Doubling the depth, and trainin with 0.95M iterations, we can improve both FID and bits/dim for both VP and sub-VP SDEs, leading to a model â€œDDPM++ cont. (deep)â€. Its FID score is 2.41, same for both VP and sub-VP SDEs. When trained with the sub-VP SDE, it can achieve a likelihood of 2.99 bits/dim. Here all likelihood values are reported for the last checkpoint during training.

H.3High resolution images
Encouraged by the success of NCSN++ on CIFAR-10, we proceed to test it on 
1024
Ã—
1024
 CelebA-HQ (Karras et al., 2018), a task that was previously only achievable by some GAN models and VQ-VAE-2 (Razavi et al., 2019). We used a batch size of 8, increased the EMA rate to 0.9999, and trained a model similar to NCSN++ with the continuous objective (Eq. 7) for around 2.4M iterations (please find the detailed architecture in our code release.) We use the PC sampler discretized at 2000 steps with the reverse diffusion predictor, one Langevin step per predictor update and a signal-to-noise ratio of 0.15. The scale parameter for the random Fourier feature embeddings is fixed to 16. We use the â€œinput skipâ€ progressive architecture for the input, and â€œoutput skipâ€ progressive architecture for the output. We provide samples in Fig. 12. Although these samples are not perfect (e.g., there are visible flaws on facial symmetry), we believe these results are encouraging and can demonstrate the scalability of our approach. Future work on more effective architectures are likely to significantly advance the performance of score-based generative models on this task.

Appendix IControllable generation
Consider a forward SDE with the following general form

d
â€‹
ğ±
=
ğŸ
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
d
â€‹
ğ‘¡
+
ğ†
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
d
â€‹
ğ°
,
and suppose the initial state distribution is 
ğ‘
0
â€‹
(
ğ±
â€‹
(
0
)
âˆ£
ğ²
)
. The density at time 
ğ‘¡
 is 
ğ‘
ğ‘¡
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
âˆ£
ğ²
)
 when conditioned on 
ğ²
. Therefore, using Anderson (1982), the reverse-time SDE is given by

d
â€‹
ğ±
=
{
ğŸ
â€‹
(
ğ±
,
ğ‘¡
)
âˆ’
âˆ‡
â‹…
[
ğ†
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
ğ†
â€‹
(
ğ±
,
ğ‘¡
)
ğ–³
]
âˆ’
ğ†
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
ğ†
â€‹
(
ğ±
,
ğ‘¡
)
ğ–³
â€‹
âˆ‡
ğ±
log
â¡
ğ‘
ğ‘¡
â€‹
(
ğ±
âˆ£
ğ²
)
}
â€‹
d
â€‹
ğ‘¡
+
ğ†
â€‹
(
ğ±
,
ğ‘¡
)
â€‹
d
â€‹
ğ°
Â¯
.
(48)
Since 
ğ‘
ğ‘¡
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
âˆ£
ğ²
)
âˆ
ğ‘
ğ‘¡
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
)
â€‹
ğ‘
â€‹
(
ğ²
âˆ£
ğ±
â€‹
(
ğ‘¡
)
)
, the score 
âˆ‡
ğ±
log
â¡
ğ‘
ğ‘¡
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
âˆ£
ğ²
)
 can be computed easily by

âˆ‡
ğ±
log
â¡
ğ‘
ğ‘¡
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
âˆ£
ğ²
)
=
âˆ‡
ğ±
log
â¡
ğ‘
ğ‘¡
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
)
+
âˆ‡
ğ±
log
â¡
ğ‘
â€‹
(
ğ²
âˆ£
ğ±
â€‹
(
ğ‘¡
)
)
.
(49)
This subsumes the conditional reverse-time SDE in Eq. 14 as a special case. All sampling methods we have discussed so far can be applied to the conditional reverse-time SDE for sample generation.

I.1Class-conditional sampling
When 
ğ²
 represents class labels, we can train a time-dependent classifier 
ğ‘
ğ‘¡
â€‹
(
ğ²
âˆ£
ğ±
â€‹
(
ğ‘¡
)
)
 for class-conditional sampling. Since the forward SDE is tractable, we can easily create a pair of training data 
(
ğ±
â€‹
(
ğ‘¡
)
,
ğ²
)
 by first sampling 
(
ğ±
â€‹
(
0
)
,
ğ²
)
 from a dataset and then obtaining 
ğ±
â€‹
(
ğ‘¡
)
âˆ¼
ğ‘
0
â€‹
ğ‘¡
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
âˆ£
ğ±
â€‹
(
0
)
)
. Afterwards, we may employ a mixture of cross-entropy losses over different time steps, like Eq. 7, to train the time-dependent classifier 
ğ‘
ğ‘¡
â€‹
(
ğ²
âˆ£
ğ±
â€‹
(
ğ‘¡
)
)
.

To test this idea, we trained a Wide ResNet (Zagoruyko & Komodakis, 2016) (Wide-ResNet-28-10) on CIFAR-10 with VE perturbations. The classifier is conditioned on 
log
â¡
ğœ
ğ‘–
 using random Fourier features (Tancik et al., 2020), and the training objective is a simple sum of cross-entropy losses sampled at different scales. We provide a plot to show the accuracy of this classifier over noise scales in Fig. 13. The score-based model is an unconditional NCSN++ (4 blocks/resolution) in Table 3, and we generate samples using the PC algorithm with 2000 discretization steps. The class-conditional samples are provided in Fig. 4, and an extended set of conditional samples is given in Fig. 13.

Refer to caption
Refer to caption
Refer to caption
Refer to caption
Refer to caption
Refer to caption
Refer to caption
Refer to caption
Refer to caption
Refer to caption
Refer to caption
Figure 13:Class-conditional image generation by solving the conditional reverse-time SDE with PC. The curve shows the accuracy of our noise-conditional classifier over different noise scales.
I.2Imputation
Imputation is a special case of conditional sampling. Denote by 
Î©
â€‹
(
ğ±
)
 and 
Î©
Â¯
â€‹
(
ğ±
)
 the known and unknown dimensions of 
ğ±
 respectively, and let 
ğŸ
Î©
Â¯
â€‹
(
â‹…
,
ğ‘¡
)
 and 
ğ†
Î©
Â¯
â€‹
(
â‹…
,
ğ‘¡
)
 denote 
ğŸ
â€‹
(
â‹…
,
ğ‘¡
)
 and 
ğ†
â€‹
(
â‹…
,
ğ‘¡
)
 restricted to the unknown dimensions. For VE/VP SDEs, the drift coefficient 
ğŸ
â€‹
(
â‹…
,
ğ‘¡
)
 is element-wise, and the diffusion coefficient 
ğ†
â€‹
(
â‹…
,
ğ‘¡
)
 is diagonal. When 
ğŸ
â€‹
(
â‹…
,
ğ‘¡
)
 is element-wise, 
ğŸ
Î©
Â¯
â€‹
(
â‹…
,
ğ‘¡
)
 denotes the same element-wise function applied only to the unknown dimensions. When 
ğ†
â€‹
(
â‹…
,
ğ‘¡
)
 is diagonal, 
ğ†
Î©
Â¯
â€‹
(
â‹…
,
ğ‘¡
)
 denotes the sub-matrix restricted to unknown dimensions.

For imputation, our goal is to sample from 
ğ‘
â€‹
(
Î©
Â¯
â€‹
(
ğ±
â€‹
(
0
)
)
âˆ£
Î©
â€‹
(
ğ±
â€‹
(
0
)
)
=
ğ²
)
. Define a new diffusion process 
ğ³
â€‹
(
ğ‘¡
)
=
Î©
Â¯
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
)
, and note that the SDE for 
ğ³
â€‹
(
ğ‘¡
)
 can be written as

d
â€‹
ğ³
=
ğŸ
Î©
Â¯
â€‹
(
ğ³
,
ğ‘¡
)
â€‹
d
â€‹
ğ‘¡
+
ğ†
Î©
Â¯
â€‹
(
ğ³
,
ğ‘¡
)
â€‹
d
â€‹
ğ°
.
The reverse-time SDE, conditioned on 
Î©
â€‹
(
ğ±
â€‹
(
0
)
)
=
ğ²
, is given by

d
ğ³
=
{
ğŸ
Î©
Â¯
(
ğ³
,
ğ‘¡
)
âˆ’
âˆ‡
â‹…
[
ğ†
Î©
Â¯
(
ğ³
,
ğ‘¡
)
ğ†
Î©
Â¯
(
ğ³
,
ğ‘¡
)
ğ–³
]
âˆ’
ğ†
Î©
Â¯
(
ğ³
,
ğ‘¡
)
ğ†
Î©
Â¯
(
ğ³
,
ğ‘¡
)
ğ–³
âˆ‡
ğ³
log
ğ‘
ğ‘¡
(
ğ³
âˆ£
Î©
(
ğ³
(
0
)
)
=
ğ²
)
}
d
ğ‘¡
+
ğ†
Î©
Â¯
(
ğ³
,
ğ‘¡
)
d
ğ°
Â¯
.
Although 
ğ‘
ğ‘¡
â€‹
(
ğ³
â€‹
(
ğ‘¡
)
âˆ£
Î©
â€‹
(
ğ±
â€‹
(
0
)
)
=
ğ²
)
 is in general intractable, it can be approximated. Let 
ğ´
 denote the event 
Î©
â€‹
(
ğ±
â€‹
(
0
)
)
=
ğ²
. We have

ğ‘
ğ‘¡
â€‹
(
ğ³
â€‹
(
ğ‘¡
)
âˆ£
Î©
â€‹
(
ğ±
â€‹
(
0
)
)
=
ğ²
)
=
ğ‘
ğ‘¡
â€‹
(
ğ³
â€‹
(
ğ‘¡
)
âˆ£
ğ´
)
=
âˆ«
ğ‘
ğ‘¡
â€‹
(
ğ³
â€‹
(
ğ‘¡
)
âˆ£
Î©
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
)
,
ğ´
)
â€‹
ğ‘
ğ‘¡
â€‹
(
Î©
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
)
âˆ£
ğ´
)
â€‹
d
Î©
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
)
=
ğ”¼
ğ‘
ğ‘¡
â€‹
(
Î©
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
)
âˆ£
ğ´
)
â€‹
[
ğ‘
ğ‘¡
â€‹
(
ğ³
â€‹
(
ğ‘¡
)
âˆ£
Î©
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
)
,
ğ´
)
]
â‰ˆ
ğ”¼
ğ‘
ğ‘¡
â€‹
(
Î©
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
)
âˆ£
ğ´
)
â€‹
[
ğ‘
ğ‘¡
â€‹
(
ğ³
â€‹
(
ğ‘¡
)
âˆ£
Î©
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
)
)
]
â‰ˆ
ğ‘
ğ‘¡
â€‹
(
ğ³
â€‹
(
ğ‘¡
)
âˆ£
Î©
^
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
)
)
,
where 
Î©
^
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
)
 is a random sample from 
ğ‘
ğ‘¡
â€‹
(
Î©
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
)
âˆ£
ğ´
)
, which is typically a tractable distribution. Therefore,

âˆ‡
ğ³
log
â¡
ğ‘
ğ‘¡
â€‹
(
ğ³
â€‹
(
ğ‘¡
)
âˆ£
Î©
â€‹
(
ğ±
â€‹
(
0
)
)
=
ğ²
)
â‰ˆ
âˆ‡
ğ³
log
â¡
ğ‘
ğ‘¡
â€‹
(
ğ³
â€‹
(
ğ‘¡
)
âˆ£
Î©
^
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
)
)
=
âˆ‡
ğ³
log
â¡
ğ‘
ğ‘¡
â€‹
(
[
ğ³
â€‹
(
ğ‘¡
)
;
Î©
^
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
)
]
)
,
where 
[
ğ³
â€‹
(
ğ‘¡
)
;
Î©
^
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
)
]
 denotes a vector 
ğ®
â€‹
(
ğ‘¡
)
 such that 
Î©
â€‹
(
ğ®
â€‹
(
ğ‘¡
)
)
=
Î©
^
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
)
 and 
Î©
Â¯
â€‹
(
ğ®
â€‹
(
ğ‘¡
)
)
=
ğ³
â€‹
(
ğ‘¡
)
, and the identity holds because 
âˆ‡
ğ³
log
â¡
ğ‘
ğ‘¡
â€‹
(
[
ğ³
â€‹
(
ğ‘¡
)
;
Î©
^
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
)
]
)
=
âˆ‡
ğ³
log
â¡
ğ‘
ğ‘¡
â€‹
(
ğ³
â€‹
(
ğ‘¡
)
âˆ£
Î©
^
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
)
)
+
âˆ‡
ğ³
log
â¡
ğ‘
â€‹
(
Î©
^
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
)
)
=
âˆ‡
ğ³
log
â¡
ğ‘
ğ‘¡
â€‹
(
ğ³
â€‹
(
ğ‘¡
)
âˆ£
Î©
^
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
)
)
.

We provided an extended set of inpainting results in Figs. 14 and 15.

I.3Colorization
Colorization is a special case of imputation, except that the known data dimensions are coupled. We can decouple these data dimensions by using an orthogonal linear transformation to map the gray-scale image to a separate channel in a different space, and then perform imputation to complete the other channels before transforming everything back to the original image space. The orthogonal matrix we used to decouple color channels is

(
0.577
âˆ’
0.816
0
0.577
0.408
0.707
0.577
0.408
âˆ’
0.707
)
.
Because the transformations are all orthogonal matrices, the standard Wiener process 
ğ°
â€‹
(
ğ‘¡
)
 will still be a standard Wiener process in the transformed space, allowing us to build an SDE and use the same imputation method in Section I.2. We provide an extended set of colorization results in Figs. 16 and 17.

I.4Solving general inverse problems
Suppose we have two random variables 
ğ±
 and 
ğ²
, and we know the forward process of generating 
ğ²
 from 
ğ±
, given by 
ğ‘
â€‹
(
ğ²
âˆ£
ğ±
)
. The inverse problem is to obtain 
ğ±
 from 
ğ²
, that is, generating samples from 
ğ‘
â€‹
(
ğ±
âˆ£
ğ²
)
. In principle, we can estimate the prior distribution 
ğ‘
â€‹
(
ğ±
)
 and obtain 
ğ‘
â€‹
(
ğ±
âˆ£
ğ²
)
 using Bayesâ€™ rule: 
ğ‘
â€‹
(
ğ±
âˆ£
ğ²
)
=
ğ‘
â€‹
(
ğ±
)
â€‹
ğ‘
â€‹
(
ğ²
âˆ£
ğ±
)
/
ğ‘
â€‹
(
ğ²
)
. In practice, however, both estimating the prior and performing Bayesian inference are non-trivial.

Leveraging Eq. 48, score-based generative models provide one way to solve the inverse problem. Suppose we have a diffusion process 
{
ğ±
â€‹
(
ğ‘¡
)
}
ğ‘¡
=
0
ğ‘‡
 generated by perturbing 
ğ±
 with an SDE, and a time-dependent score-based model 
ğ¬
ğœ½
â£
âˆ—
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
,
ğ‘¡
)
 trained to approximate 
âˆ‡
ğ±
log
â¡
ğ‘
ğ‘¡
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
)
. Once we have an estimate of 
âˆ‡
ğ±
log
â¡
ğ‘
ğ‘¡
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
âˆ£
ğ²
)
, we can simulate the reverse-time SDE in Eq. 48 to sample from 
ğ‘
0
â€‹
(
ğ±
â€‹
(
0
)
âˆ£
ğ²
)
=
ğ‘
â€‹
(
ğ±
âˆ£
ğ²
)
. To obtain this estimate, we first observe that

âˆ‡
ğ±
log
â¡
ğ‘
ğ‘¡
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
âˆ£
ğ²
)
=
âˆ‡
ğ±
log
â€‹
âˆ«
ğ‘
ğ‘¡
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
âˆ£
ğ²
â€‹
(
ğ‘¡
)
,
ğ²
)
â€‹
ğ‘
â€‹
(
ğ²
â€‹
(
ğ‘¡
)
âˆ£
ğ²
)
â€‹
d
ğ²
â€‹
(
ğ‘¡
)
,
where 
ğ²
â€‹
(
ğ‘¡
)
 is defined via 
ğ±
â€‹
(
ğ‘¡
)
 and the forward process 
ğ‘
â€‹
(
ğ²
â€‹
(
ğ‘¡
)
âˆ£
ğ±
â€‹
(
ğ‘¡
)
)
. Now assume two conditions:

â€¢ 
ğ‘
â€‹
(
ğ²
â€‹
(
ğ‘¡
)
âˆ£
ğ²
)
 is tractable. We can often derive this distribution from the interaction between the forward process and the SDE, like in the case of image imputation and colorization.
â€¢ 
ğ‘
ğ‘¡
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
âˆ£
ğ²
â€‹
(
ğ‘¡
)
,
ğ²
)
â‰ˆ
ğ‘
ğ‘¡
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
âˆ£
ğ²
â€‹
(
ğ‘¡
)
)
. For small 
ğ‘¡
, 
ğ²
â€‹
(
ğ‘¡
)
 is almost the same as 
ğ²
 so the approximation holds. For large 
ğ‘¡
, 
ğ²
 becomes further away from 
ğ±
â€‹
(
ğ‘¡
)
 in the Markov chain, and thus have smaller impact on 
ğ±
â€‹
(
ğ‘¡
)
. Moreover, the approximation error for large 
ğ‘¡
 matter less for the final sample, since it is used early in the sampling process.
Given these two assumptions, we have

âˆ‡
ğ±
log
â¡
ğ‘
ğ‘¡
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
âˆ£
ğ²
)
â‰ˆ
âˆ‡
ğ±
log
â€‹
âˆ«
ğ‘
ğ‘¡
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
âˆ£
ğ²
â€‹
(
ğ‘¡
)
)
â€‹
ğ‘
â€‹
(
ğ²
â€‹
(
ğ‘¡
)
âˆ£
ğ²
)
â€‹
d
ğ²
â‰ˆ
âˆ‡
ğ±
log
â¡
ğ‘
ğ‘¡
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
âˆ£
ğ²
^
â€‹
(
ğ‘¡
)
)
=
âˆ‡
ğ±
log
â¡
ğ‘
ğ‘¡
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
)
+
âˆ‡
ğ±
log
â¡
ğ‘
ğ‘¡
â€‹
(
ğ²
^
â€‹
(
ğ‘¡
)
âˆ£
ğ±
â€‹
(
ğ‘¡
)
)
â‰ˆ
ğ¬
ğœ½
âˆ—
â€‹
(
ğ±
â€‹
(
ğ‘¡
)
,
ğ‘¡
)
+
âˆ‡
ğ±
log
â¡
ğ‘
ğ‘¡
â€‹
(
ğ²
^
â€‹
(
ğ‘¡
)
âˆ£
ğ±
â€‹
(
ğ‘¡
)
)
,
(50)
where 
ğ²
^
â€‹
(
ğ‘¡
)
 is a sample from 
ğ‘
â€‹
(
ğ²
â€‹
(
ğ‘¡
)
âˆ£
ğ²
)
. Now we can plug Eq. 50 into Eq. 48 and solve the resulting reverse-time SDE to generate samples from 
ğ‘
â€‹
(
ğ±
âˆ£
ğ²
)
.

Refer to caption
Figure 14:Extended inpainting results for 
256
Ã—
256
 bedroom images.
Refer to caption
Figure 15:Extended inpainting results for 
256
Ã—
256
 church images.
Refer to caption
Figure 16:Extended colorization results for 
256
Ã—
256
 bedroom images.
Refer to caption
Figure 17:Extended colorization results for 
256
Ã—
256
 church images.
â—„ ar5iv homepage Feeling
lucky? Conversion
report Report
an issue View original
on arXivâ–º
Copyright Privacy Policy Generated on Thu Mar 7 03:03:12 2024 by LaTeXMLMascot Sammy